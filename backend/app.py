"""Flask backend for Excel Intelligent Agent System with RAG capabilities."""

import os
import asyncio
import sys
import tempfile
import uuid
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional

from flask import Flask, request, jsonify, render_template, g
from flask_cors import CORS
from werkzeug.utils import secure_filename
import logging
import time
import json

# Import our Excel to HTML utility
from utils.excel_to_html import excel_to_html, get_excel_sheets, excel_sheet_to_html, get_excel_info

# Add src directory to Python path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

# Try to import the agent system with fallback
try:
    from excel_agent.core.orchestrator import Orchestrator
    from excel_agent.mcp.agent_configs import initialize_mcp_system, initialize_all_agent_mcp
    from excel_agent.utils.config import config
    AGENT_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Agent system not available: {e}")
    AGENT_AVAILABLE = False
    # Create a mock config for fallback
    class MockConfig:
        def get_llm_params(self):
            return {'model': 'mock', 'temperature': 0.7, 'max_tokens': None}
        def update_llm_params(self, **params):
            pass
        def get_available_models(self):
            return {'llm_models': ['mock-model']}
        def reset_llm_params_to_default(self):
            pass
    config = MockConfig()

app = Flask(__name__)
CORS(app)

# Configuration
app.config['MAX_CONTENT_LENGTH'] = 50 * 1024 * 1024  # 50MB max file size
app.config['UPLOAD_FOLDER'] = tempfile.mkdtemp()

# Setup detailed logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(name)s | %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('excel_agent_api.log')
    ]
)

api_logger = logging.getLogger('excel_agent.api')


def log_request_response(f):
    """Decorator to log API requests and responses."""
    def decorated_function(*args, **kwargs):
        # Log request
        request_id = str(uuid.uuid4())[:8]
        g.request_id = request_id
        
        start_time = time.time()
        
        # Log request details
        api_logger.info(f"üì• [Request {request_id}] {request.method} {request.path}")
        api_logger.info(f"üì• [Request {request_id}] Remote: {request.remote_addr}")
        api_logger.info(f"üì• [Request {request_id}] User-Agent: {request.headers.get('User-Agent', 'N/A')}")
        
        # Log request body (if present and not too large)
        if request.is_json and request.content_length and request.content_length < 10000:
            try:
                request_data = request.get_json()
                # Mask sensitive data
                if isinstance(request_data, dict) and 'query' in request_data:
                    log_data = request_data.copy()
                    if len(log_data['query']) > 200:
                        log_data['query'] = log_data['query'][:200] + "... (truncated)"
                    api_logger.debug(f"üì• [Request {request_id}] Body: {json.dumps(log_data, ensure_ascii=False)}")
                else:
                    api_logger.debug(f"üì• [Request {request_id}] Body: {json.dumps(request_data, ensure_ascii=False)}")
            except Exception as e:
                api_logger.debug(f"üì• [Request {request_id}] Body: <unable to parse: {e}>")
        elif request.files:
            files_info = []
            for file_key, file_obj in request.files.items():
                files_info.append(f"{file_key}: {file_obj.filename} ({file_obj.content_length} bytes)")
            api_logger.info(f"üì• [Request {request_id}] Files: {', '.join(files_info)}")
        
        try:
            # Execute the function
            response = f(*args, **kwargs)
            
            # Log response
            end_time = time.time()
            response_time = end_time - start_time
            
            if hasattr(response, 'status_code'):
                status_code = response.status_code
                api_logger.info(f"üì§ [Response {request_id}] Status: {status_code}, Time: {response_time:.3f}s")
                
                # Log response body (if JSON and not too large)
                if hasattr(response, 'is_json') and response.is_json and response.content_length and response.content_length < 10000:
                    try:
                        response_data = response.get_json()
                        if isinstance(response_data, dict):
                            log_data = response_data.copy()
                            # Truncate long responses
                            if 'response' in log_data and len(str(log_data['response'])) > 500:
                                log_data['response'] = str(log_data['response'])[:500] + "... (truncated)"
                            if 'generated_code' in log_data and len(str(log_data['generated_code'])) > 500:
                                log_data['generated_code'] = str(log_data['generated_code'])[:500] + "... (truncated)"
                            api_logger.debug(f"üì§ [Response {request_id}] Body: {json.dumps(log_data, ensure_ascii=False)}")
                    except Exception as e:
                        api_logger.debug(f"üì§ [Response {request_id}] Body: <unable to parse: {e}>")
            else:
                api_logger.info(f"üì§ [Response {request_id}] Time: {response_time:.3f}s")
            
            return response
            
        except Exception as e:
            # Log error
            end_time = time.time()
            response_time = end_time - start_time
            api_logger.error(f"‚ùå [Error {request_id}] {str(e)}, Time: {response_time:.3f}s")
            raise
    
    decorated_function.__name__ = f.__name__
    return decorated_function

# Allowed file extensions
ALLOWED_EXTENSIONS = {'xlsx', 'xls', 'xlsm'}

# Global variables
orchestrator = None
uploaded_files = {}
mcp_initialized = False


def allowed_file(filename):
    """Check if file extension is allowed."""
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS


async def initialize_agent_system():
    """Initialize the agent system with MCP capabilities."""
    global orchestrator, mcp_initialized
    
    if not AGENT_AVAILABLE:
        return False
    
    try:
        print("Initializing Agent System...")
        
        # Initialize MCP system
        initialize_mcp_system()
        await initialize_all_agent_mcp()
        mcp_initialized = True
        
        # Initialize orchestrator
        orchestrator = Orchestrator()
        
        print("Agent System initialized successfully!")
        return True
        
    except Exception as e:
        print(f"Failed to initialize agent system: {e}")
        return False


def run_async(coro):
    """Run async function in sync context."""
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # If loop is already running, create a new task
            return loop.create_task(coro)
        else:
            return loop.run_until_complete(coro)
    except RuntimeError:
        # No event loop, create a new one
        return asyncio.run(coro)


@app.route('/')
def index():
    """Serve the main page."""
    return render_template('index.html')


@app.route('/chat')
def chat():
    """Serve the new chat interface."""
    return render_template('chat.html')


@app.route('/api/status')
@log_request_response
def get_status():
    """Get system status."""
    status = {
        'agent_available': AGENT_AVAILABLE,
        'mcp_initialized': mcp_initialized,
        'orchestrator_ready': orchestrator is not None,
        'upload_folder': app.config['UPLOAD_FOLDER'],
        'max_file_size': app.config['MAX_CONTENT_LENGTH'],
        'allowed_extensions': list(ALLOWED_EXTENSIONS)
    }
    
    if orchestrator and mcp_initialized:
        try:
            # Get MCP status if available
            mcp_status = asyncio.run(orchestrator.get_mcp_status())
            status['mcp_status'] = mcp_status
        except Exception as e:
            status['mcp_error'] = str(e)
    
    return jsonify(status)


@app.route('/api/initialize', methods=['POST'])
@log_request_response
def initialize_system():
    """Initialize the agent system."""
    global orchestrator, mcp_initialized
    
    try:
        if AGENT_AVAILABLE:
            print("Attempting to initialize agent system...")
            success = asyncio.run(initialize_agent_system())
            
            if success:
                return jsonify({
                    'success': True,
                    'message': 'ÂçèË∞ÉÂô®Á≥ªÁªüÂàùÂßãÂåñÊàêÂäü',
                    'orchestrator_ready': orchestrator is not None,
                    'mcp_initialized': mcp_initialized
                })
            else:
                return jsonify({
                    'success': False,
                    'message': 'ÂçèË∞ÉÂô®Á≥ªÁªüÂàùÂßãÂåñÂ§±Ë¥•ÔºåËøêË°åÂú®ÊºîÁ§∫Ê®°Âºè',
                    'orchestrator_ready': False,
                    'mcp_initialized': False
                }), 500
        else:
            return jsonify({
                'success': False,
                'message': 'AgentÁ≥ªÁªü‰∏çÂèØÁî®ÔºåÁº∫Â∞ëÂøÖË¶Å‰æùËµñ',
                'orchestrator_ready': False,
                'mcp_initialized': False
            }), 503
    
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'ÂàùÂßãÂåñÂ§±Ë¥•: {str(e)}',
            'orchestrator_ready': False,
            'mcp_initialized': False
        }), 500


@app.route('/api/upload', methods=['POST'])
@log_request_response
def upload_file():
    """Handle single file upload."""
    try:
        if 'file' not in request.files:
            return jsonify({'error': 'No file provided'}), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({'error': 'No file selected'}), 400
        
        if not allowed_file(file.filename):
            return jsonify({'error': 'Invalid file type. Please upload Excel files (.xlsx, .xls, .xlsm)'}), 400
        
        # Generate unique filename
        file_id = str(uuid.uuid4())
        filename = secure_filename(file.filename)
        file_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{file_id}_{filename}")
        
        # Save file
        file.save(file_path)
        
        # Store file info
        file_info = {
            'file_id': file_id,
            'original_name': filename,
            'file_path': file_path,
            'size': os.path.getsize(file_path),
            'upload_time': datetime.now().isoformat(),
            'processed': False
        }
        
        uploaded_files[file_id] = file_info
        
        return jsonify({
            'success': True,
            'file_id': file_id,
            'filename': filename,
            'size': file_info['size'],
            'message': 'File uploaded successfully'
        })
        
    except Exception as e:
        return jsonify({'error': f'Upload failed: {str(e)}'}), 500


@app.route('/api/upload/batch', methods=['POST'])
@log_request_response
def upload_batch_files():
    """Handle multiple file upload."""
    try:
        if 'files' not in request.files:
            return jsonify({'error': 'No files provided'}), 400
        
        files = request.files.getlist('files')
        if not files or all(f.filename == '' for f in files):
            return jsonify({'error': 'No files selected'}), 400
        
        uploaded_file_infos = []
        file_ids = []
        
        for file in files:
            if file.filename == '':
                continue
                
            if not allowed_file(file.filename):
                api_logger.warning(f"Skipping invalid file: {file.filename}")
                continue
            
            # Generate unique filename
            file_id = str(uuid.uuid4())
            filename = secure_filename(file.filename)
            file_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{file_id}_{filename}")
            
            # Save file
            file.save(file_path)
            
            # Store file info
            file_info = {
                'file_id': file_id,
                'original_name': filename,
                'file_path': file_path,
                'size': os.path.getsize(file_path),
                'upload_time': datetime.now().isoformat(),
                'processed': False
            }
            
            uploaded_files[file_id] = file_info
            uploaded_file_infos.append(file_info)
            file_ids.append(file_id)
        
        if not uploaded_file_infos:
            return jsonify({'error': 'No valid Excel files were uploaded'}), 400
        
        return jsonify({
            'success': True,
            'file_ids': file_ids,
            'files': [{
                'file_id': info['file_id'],
                'filename': info['original_name'],
                'size': info['size']
            } for info in uploaded_file_infos],
            'message': f'{len(uploaded_file_infos)} files uploaded successfully'
        })
        
    except Exception as e:
        return jsonify({'error': f'Batch upload failed: {str(e)}'}), 500


@app.route('/api/process/<file_id>', methods=['POST'])
@log_request_response
def process_file(file_id):
    """Process uploaded file using the agent system."""
    try:
        if file_id not in uploaded_files:
            return jsonify({'error': 'File not found'}), 404
        
        file_info = uploaded_files[file_id]
        
        if not AGENT_AVAILABLE or not orchestrator:
            # Return mock response if agent system not available
            return jsonify({
                'success': True,
                'file_id': file_id,
                'processed': True,
                'mock_mode': True,
                'steps': [
                    {'step': 'upload', 'status': 'completed', 'message': 'File uploaded'},
                    {'step': 'ingest', 'status': 'completed', 'message': 'File parsed (mock)'},
                    {'step': 'profile', 'status': 'completed', 'message': 'Data analyzed (mock)'},
                    {'step': 'ready', 'status': 'completed', 'message': 'Ready for queries (mock)'}
                ],
                'metadata': {
                    'sheets': ['Sheet1', 'Data', 'Summary'],
                    'total_rows': 1234,
                    'total_columns': 8,
                    'file_size': file_info['size']
                },
                'preview': {
                    'columns': ['Product', 'Sales', 'Quantity', 'Date', 'Category', 'Price', 'Profit', 'Region'],
                    'sample_data': [
                        ['Laptop', 15000, 5, '2024-01-15', 'Electronics', 3000, 2000, 'North'],
                        ['Phone', 8000, 10, '2024-01-16', 'Electronics', 800, 1200, 'South'],
                        ['Tablet', 6000, 8, '2024-01-17', 'Electronics', 750, 800, 'East']
                    ]
                }
            })
        
        # Process with real agent system (direct file ingestion without intent parsing)
        async def process_with_agents():
            file_path = file_info['file_path']
            
            # Import file ingest agent and required models
            from excel_agent.agents.file_ingest import FileIngestAgent
            from excel_agent.models.agents import FileIngestRequest
            from excel_agent.models.base import AgentStatus
            
            # Create file ingest agent and process file directly
            file_agent = FileIngestAgent()
            
            ingest_request = FileIngestRequest(
                agent_id="FileIngestAgent",
                file_path=file_path,
                context={'file_id': file_id, 'processing_mode': 'file_upload'}
            )
            
            async with file_agent:
                ingest_response = await file_agent.execute_with_timeout(ingest_request)
            
            # Prepare result without intent parsing
            if ingest_response.status == AgentStatus.SUCCESS:
                return {
                    'status': 'success',
                    'workflow_type': 'file_ingest_only',
                    'steps': [
                        {'step': 'file_ingest', 'status': 'success', 'result': ingest_response.result}
                    ],
                    'file_metadata': ingest_response.result,
                    'sheets': ingest_response.sheets,
                    'file_id': ingest_response.file_id,
                    'timestamp': datetime.now().isoformat()
                }
            else:
                return {
                    'status': 'failed',
                    'error_message': 'File ingestion failed',
                    'error_log': ingest_response.error_log,
                    'timestamp': datetime.now().isoformat()
                }
        
        # Run async processing
        result = run_async(process_with_agents())
        
        # Update file info
        file_info['processed'] = True
        file_info['process_result'] = result
        
        return jsonify({
            'success': True,
            'file_id': file_id,
            'processed': True,
            'result': result
        })
        
    except Exception as e:
        return jsonify({'error': f'Processing failed: {str(e)}'}), 500


@app.route('/api/process/batch', methods=['POST'])
@log_request_response
def process_batch_files():
    """Process multiple uploaded files using the agent system."""
    try:
        data = request.get_json()
        if not data or 'file_ids' not in data:
            return jsonify({'error': 'No file IDs provided'}), 400
        
        file_ids = data['file_ids']
        if not isinstance(file_ids, list) or not file_ids:
            return jsonify({'error': 'Invalid file IDs format'}), 400
        
        # Validate all file IDs exist
        missing_files = [fid for fid in file_ids if fid not in uploaded_files]
        if missing_files:
            return jsonify({'error': f'Files not found: {missing_files}'}), 404
        
        if not AGENT_AVAILABLE or not orchestrator:
            # Return mock response if agent system not available
            processed_files = []
            for file_id in file_ids:
                file_info = uploaded_files[file_id]
                processed_files.append({
                    'file_id': file_id,
                    'filename': file_info['original_name'],
                    'processed': True,
                    'steps': [
                        {'step': 'upload', 'status': 'completed', 'message': 'File uploaded'},
                        {'step': 'ingest', 'status': 'completed', 'message': 'File parsed (mock)'},
                        {'step': 'profile', 'status': 'completed', 'message': 'Data analyzed (mock)'},
                        {'step': 'ready', 'status': 'completed', 'message': 'Ready for queries (mock)'}
                    ]
                })
                # Mark as processed
                file_info['processed'] = True
            
            return jsonify({
                'success': True,
                'processed_files': processed_files,
                'mock_mode': True,
                'message': f'{len(file_ids)} files processed successfully (mock mode)'
            })
        
        # Process with real agent system
        async def process_batch_with_agents():
            results = []
            for file_id in file_ids:
                file_info = uploaded_files[file_id]
                file_path = file_info['file_path']
                
                try:
                    # Import required components for direct file processing
                    from excel_agent.agents.file_ingest import FileIngestAgent
                    from excel_agent.models.agents import FileIngestRequest
                    from excel_agent.models.base import AgentStatus
                    
                    # Process file directly without intent parsing
                    file_agent = FileIngestAgent()
                    
                    ingest_request = FileIngestRequest(
                        agent_id="FileIngestAgent",
                        file_path=file_path,
                        context={'file_id': file_id, 'batch_processing': True}
                    )
                    
                    async with file_agent:
                        ingest_response = await file_agent.execute_with_timeout(ingest_request)
                    
                    if ingest_response.status == AgentStatus.SUCCESS:
                        result = {
                            'status': 'success',
                            'workflow_type': 'file_ingest_only',
                            'file_metadata': ingest_response.result,
                            'sheets': ingest_response.sheets,
                            'file_id': ingest_response.file_id
                        }
                    else:
                        result = {
                            'status': 'failed',
                            'error_message': 'File ingestion failed',
                            'error_log': ingest_response.error_log
                        }
                    
                    results.append({
                        'file_id': file_id,
                        'filename': file_info['original_name'],
                        'processed': True,
                        'result': result
                    })
                    
                    # Update file info
                    file_info['processed'] = True
                    file_info['process_result'] = result
                    
                except Exception as e:
                    api_logger.error(f"Failed to process file {file_id}: {str(e)}")
                    results.append({
                        'file_id': file_id,
                        'filename': file_info['original_name'],
                        'processed': False,
                        'error': str(e)
                    })
            
            return results
        
        # Run async batch processing
        results = run_async(process_batch_with_agents())
        
        # Count successful and failed processing
        successful = len([r for r in results if r.get('processed', False)])
        failed = len(results) - successful
        
        return jsonify({
            'success': True,
            'processed_files': results,
            'summary': {
                'total': len(file_ids),
                'successful': successful,
                'failed': failed
            },
            'message': f'Batch processing completed: {successful} successful, {failed} failed'
        })
        
    except Exception as e:
        return jsonify({'error': f'Batch processing failed: {str(e)}'}), 500


@app.route('/api/query', methods=['POST'])
@log_request_response
def query_data():
    """Handle user queries about the data."""
    try:
        data = request.get_json()
        file_id = data.get('file_id')
        query = data.get('query', '').strip()
        
        if not query:
            return jsonify({'error': 'Query cannot be empty'}), 400
        
        if file_id not in uploaded_files:
            return jsonify({'error': 'File not found'}), 404
        
        file_info = uploaded_files[file_id]
        
        if not AGENT_AVAILABLE or not orchestrator:
            # Return mock response if agent system not available
            mock_responses = generate_mock_response(query, file_info)
            
            # Create mock workflow steps
            mock_workflow_steps = [
                {'step': 'intent_parsing', 'status': 'success', 'agent': 'Orchestrator', 'description': 'ÊÑèÂõæËß£ÊûêÂÆåÊàê'},
                {'step': 'file_ingest', 'status': 'success', 'agent': 'FileIngestAgent', 'description': 'Êñá‰ª∂Êï∞ÊçÆÂä†ËΩΩÊàêÂäü'},
                {'step': 'column_profiling', 'status': 'success', 'agent': 'ColumnProfilingAgent', 'description': 'Êï∞ÊçÆÂàóÂàÜÊûêÂÆåÊàê'},
                {'step': 'code_generation', 'status': 'success', 'agent': 'CodeGenerationAgent', 'description': 'ÂàÜÊûê‰ª£Á†ÅÁîüÊàêÊàêÂäü'},
                {'step': 'execution', 'status': 'success', 'agent': 'ExecutionAgent', 'description': '‰ª£Á†ÅÊâßË°åÂÆåÊàê'}
            ]
            
            return jsonify({
                'success': True,
                'query': query,
                'response': mock_responses['analysis'],
                'generated_code': mock_responses['code'],
                'workflow_steps': mock_workflow_steps,
                'mock_mode': True,
                'execution_time': 2.5,
                'insights': mock_responses.get('insights', []),
                'visualizations': mock_responses.get('visualizations', [])
            })
        
        # Process with real agent system
        async def query_with_agents():
            file_path = file_info['file_path']
            
            # Use orchestrator to process query
            result = await orchestrator.process_user_request(
                user_request=query,
                file_path=file_path,
                context={
                    'file_id': file_id,
                    'previous_processing': file_info.get('process_result')
                }
            )
            
            return result
        
        # Run async query processing
        result = run_async(query_with_agents())
        
        # Check if processing was successful
        if result.get('status') == 'failed':
            api_logger.error(f"Query processing failed for file {file_id}: {result}")
            return jsonify({
                'success': False,
                'error': result.get('error', 'Query processing failed'),
                'error_message': result.get('error_message', result.get('error', 'Unknown error')),
                'error_type': result.get('error_type', 'ProcessingError'),
                'query': query,
                'file_id': file_id,
                'execution_time': result.get('execution_time', 0),
                'workflow_steps': result.get('workflow_steps', result.get('steps', [])),
                'timestamp': result.get('timestamp'),
                'debug_info': {
                    'file_path': file_info.get('file_path'),
                    'file_processed': file_info.get('processed', False),
                    'agent_available': AGENT_AVAILABLE,
                    'orchestrator_ready': orchestrator is not None
                }
            }), 500
        
        return jsonify({
            'success': True,
            'query': query,
            'response': result.get('final_result', 'Analysis completed'),
            'generated_code': result.get('generated_code', ''),
            'workflow_steps': result.get('steps', []),
            'execution_time': result.get('execution_time', 0),
            'status': result.get('status', 'completed')
        })
        
    except Exception as e:
        return jsonify({'error': f'Query failed: {str(e)}'}), 500


@app.route('/api/query/batch', methods=['POST'])
@log_request_response
def query_batch_data():
    """Handle user queries about multiple files data."""
    try:
        data = request.get_json()
        file_ids = data.get('file_ids', [])
        query = data.get('query', '').strip()
        
        if not query:
            return jsonify({'error': 'Query cannot be empty'}), 400
        
        if not isinstance(file_ids, list) or not file_ids:
            return jsonify({'error': 'No file IDs provided'}), 400
        
        # Validate all file IDs exist
        missing_files = [fid for fid in file_ids if fid not in uploaded_files]
        if missing_files:
            return jsonify({'error': f'Files not found: {missing_files}'}), 404
        
        # Get file info for all files
        file_infos = [uploaded_files[fid] for fid in file_ids]
        file_names = [info['original_name'] for info in file_infos]
        
        if not AGENT_AVAILABLE or not orchestrator:
            # Return mock response for batch query
            mock_responses = generate_mock_batch_response(query, file_infos)
            
            # Create mock workflow steps
            mock_workflow_steps = [
                {'step': 'intent_parsing', 'status': 'success', 'agent': 'Orchestrator', 'description': 'ÊÑèÂõæËß£ÊûêÂÆåÊàê'},
                {'step': 'batch_file_ingest', 'status': 'success', 'agent': 'FileIngestAgent', 'description': f'{len(file_ids)}‰∏™Êñá‰ª∂Êï∞ÊçÆÂä†ËΩΩÊàêÂäü'},
                {'step': 'data_merging', 'status': 'success', 'agent': 'DataMergeAgent', 'description': 'Êï∞ÊçÆÂêàÂπ∂ÂÆåÊàê'},
                {'step': 'column_profiling', 'status': 'success', 'agent': 'ColumnProfilingAgent', 'description': 'ÊâπÈáèÊï∞ÊçÆÂàÜÊûêÂÆåÊàê'},
                {'step': 'code_generation', 'status': 'success', 'agent': 'CodeGenerationAgent', 'description': 'ÂàÜÊûê‰ª£Á†ÅÁîüÊàêÊàêÂäü'},
                {'step': 'execution', 'status': 'success', 'agent': 'ExecutionAgent', 'description': '‰ª£Á†ÅÊâßË°åÂÆåÊàê'}
            ]
            
            return jsonify({
                'success': True,
                'query': query,
                'file_ids': file_ids,
                'file_names': file_names,
                'response': mock_responses['analysis'],
                'generated_code': mock_responses['code'],
                'workflow_steps': mock_workflow_steps,
                'mock_mode': True,
                'execution_time': 4.2,
                'insights': mock_responses.get('insights', []),
                'visualizations': mock_responses.get('visualizations', [])
            })
        
        # Process with real agent system
        async def query_batch_with_agents():
            file_paths = [info['file_path'] for info in file_infos]
            
            # Use orchestrator to process batch query
            result = await orchestrator.process_user_request(
                user_request=f"{query} (ÂàÜÊûêËøô{len(file_ids)}‰∏™Êñá‰ª∂: {', '.join(file_names)})",
                file_path=file_paths[0],  # Primary file
                context={
                    'file_ids': file_ids,
                    'file_paths': file_paths,
                    'file_names': file_names,
                    'batch_query': True,
                    'previous_processing': [info.get('process_result') for info in file_infos]
                }
            )
            
            return result
        
        # Run async batch query processing
        result = run_async(query_batch_with_agents())
        
        return jsonify({
            'success': True,
            'query': query,
            'file_ids': file_ids,
            'file_names': file_names,
            'response': result.get('final_result', 'Batch analysis completed'),
            'generated_code': result.get('generated_code', ''),
            'workflow_steps': result.get('steps', []),
            'execution_time': result.get('execution_time', 0),
            'status': result.get('status', 'completed')
        })
        
    except Exception as e:
        return jsonify({'error': f'Batch query failed: {str(e)}'}), 500


def generate_mock_batch_response(query, file_infos):
    """Generate mock responses for batch queries when agent system is not available."""
    query_lower = query.lower()
    file_count = len(file_infos)
    total_size = sum(info['size'] for info in file_infos)
    file_names = [info['original_name'] for info in file_infos]
    
    if 'ÁªüËÆ°' in query_lower or 'Âü∫Êú¨‰ø°ÊÅØ' in query_lower:
        return {
            'analysis': f"""üìä ÊâπÈáèÊï∞ÊçÆÁªüËÆ°ÂàÜÊûêÁªìÊûú

Âü∫‰∫éÊÇ®‰∏ä‰º†ÁöÑ {file_count} ‰∏™Êñá‰ª∂Ôºö
{chr(10).join([f'‚Ä¢ {name}' for name in file_names])}

üìã Êï¥‰ΩìÊï∞ÊçÆÊ¶ÇËßàÔºö
‚Ä¢ Êñá‰ª∂ÊÄªÊï∞: {file_count} ‰∏™
‚Ä¢ ÊÄªÊñá‰ª∂Â§ßÂ∞è: {total_size} bytes
‚Ä¢ ÂêàÂπ∂ÂêéÊÄªË°åÊï∞: {file_count * 1234:,} Ë°å
‚Ä¢ ÂêàÂπ∂ÂêéÊÄªÂàóÊï∞: 8 ÂàóÔºàÊ†áÂáÜÂåñÂêéÔºâ
‚Ä¢ Êï∞ÊçÆÁ±ªÂûã: 4‰∏™Êï∞ÂÄºÂàó, 3‰∏™ÊñáÊú¨Âàó, 1‰∏™Êó•ÊúüÂàó

üìà ÂêàÂπ∂Êï∞ÊçÆÁªüËÆ°Ôºö
‚Ä¢ ÈîÄÂîÆÈ¢ù: Âπ≥ÂùáÂÄº ¬•{8547 * file_count:,}, ÊúÄÂ§ßÂÄº ¬•{25000 * file_count:,}, ÊúÄÂ∞èÂÄº ¬•1,200
‚Ä¢ Êï∞Èáè: Âπ≥ÂùáÂÄº {15.6 * file_count:.1f}, ÊúÄÂ§ßÂÄº {100 * file_count}, ÊúÄÂ∞èÂÄº 1
‚Ä¢ Âà©Ê∂¶Áéá: Âπ≥ÂùáÂÄº 23.4%, Ê†áÂáÜÂ∑Æ 8.9%

üîç Êï∞ÊçÆË¥®ÈáèÔºö
‚Ä¢ ÂÆåÊï¥ÊÄß: 98.7% ({16 * file_count}‰∏™Áº∫Â§±ÂÄº)
‚Ä¢ ÈáçÂ§çËÆ∞ÂΩï: {3 * file_count}Êù°
‚Ä¢ ÂºÇÂ∏∏ÂÄº: Âú®ÈîÄÂîÆÈ¢ùÂàóÂèëÁé∞{5 * file_count}‰∏™ÊΩúÂú®ÂºÇÂ∏∏ÂÄº

üí° Ë∑®Êñá‰ª∂Ê¥ûÂØüÔºö
1. ÊâÄÊúâÊñá‰ª∂ÁöÑÁîµÂ≠ê‰∫ßÂìÅÁ±ªÂà´ÈîÄÂîÆÈ¢ùÂç†ÊÄªÈîÄÂîÆÈ¢ùÁöÑ68%
2. Êñá‰ª∂Èó¥Êï∞ÊçÆ‰∏ÄËá¥ÊÄßËâØÂ•ΩÔºåÊ†ºÂºèÁªü‰∏Ä
3. ÂêàÂπ∂ÂêéÊï∞ÊçÆÈáèÂ¢ûÂä†‰∫Ü{file_count}ÂÄçÔºåÂàÜÊûêÊõ¥ÂÖ®Èù¢
4. ÂèëÁé∞Ë∑®Êñá‰ª∂ÁöÑÊó∂Èó¥Â∫èÂàóÊ®°Âºè""",
            
            'code': f'''import pandas as pd
import numpy as np

# ËØªÂèñÂπ∂ÂêàÂπ∂Â§ö‰∏™ExcelÊñá‰ª∂
file_list = {[info['original_name'] for info in file_infos]}
all_data = []

for file in file_list:
    df = pd.read_excel(file)
    df['source_file'] = file  # Ê∑ªÂä†Ê∫êÊñá‰ª∂Ê†áËØÜ
    all_data.append(df)
    print(f"Âä†ËΩΩÊñá‰ª∂: {{file}}, Ë°åÊï∞: {{df.shape[0]}}, ÂàóÊï∞: {{df.shape[1]}}")

# ÂêàÂπ∂ÊâÄÊúâÊï∞ÊçÆ
combined_df = pd.concat(all_data, ignore_index=True)
print(f"\\nÂêàÂπ∂ÂêéÊï∞ÊçÆÂΩ¢Áä∂: {{combined_df.shape}}")

# Âü∫Êú¨ÁªüËÆ°‰ø°ÊÅØ
print("\\nÊï∞ÊçÆÁ±ªÂûã:")
print(combined_df.dtypes)

# Êï∞ÂÄºÂàóÁªüËÆ°
numeric_cols = combined_df.select_dtypes(include=[np.number]).columns
print("\\nÊï∞ÂÄºÂàóÁªüËÆ°:")
print(combined_df[numeric_cols].describe())

# ÊåâÊ∫êÊñá‰ª∂ÂàÜÁªÑÁªüËÆ°
print("\\nÂêÑÊñá‰ª∂ÁªüËÆ°:")
print(combined_df.groupby('source_file').size())

# Áº∫Â§±ÂÄºÊ£ÄÊü•
print("\\nÁº∫Â§±ÂÄºÁªüËÆ°:")
print(combined_df.isnull().sum())'''
        }
    
    else:
        return {
            'analysis': f"""ü§ñ ÊâπÈáèAIÂàÜÊûêÂõûÂ§ç

ÈíàÂØπÊÇ®ÁöÑÈóÆÈ¢òÔºö"{query}"

Âü∫‰∫é‰∏ä‰º†ÁöÑ {file_count} ‰∏™Êñá‰ª∂Ôºö
{chr(10).join([f'‚Ä¢ {name}' for name in file_names])}

üìä ÊâπÈáèÊï∞ÊçÆÊ¶ÇÂÜµÔºö
‚Ä¢ Êñá‰ª∂ÊÄªÊï∞: {file_count} ‰∏™
‚Ä¢ ÊÄªÊñá‰ª∂Â§ßÂ∞è: {total_size} bytes
‚Ä¢ È¢ÑËÆ°ÂêàÂπ∂ÂêéÊï∞ÊçÆÈáè: {file_count * 1234:,} Ë°å

üîç Âª∫ËÆÆÁöÑÊâπÈáèÂàÜÊûêÊñπÂêëÔºö
1. Ë∑®Êñá‰ª∂Êï∞ÊçÆ‰∏ÄËá¥ÊÄßÊ£ÄÊü•
2. ÂêàÂπ∂Êï∞ÊçÆÁöÑÁªüËÆ°ÂàÜÊûê
3. Êñá‰ª∂Èó¥Êï∞ÊçÆÂ∑ÆÂºÇÂØπÊØî
4. ÊâπÈáèÂºÇÂ∏∏ÂÄºÊ£ÄÊµã
5. Ë∑®Êñá‰ª∂Ë∂ãÂäøÂàÜÊûê
6. ÁªºÂêàÂèØËßÜÂåñÊä•Âëä

üí° ÊÇ®ÂèØ‰ª•Â∞ùËØï‰ª•‰∏ãÊâπÈáèÂàÜÊûêÈóÆÈ¢òÔºö
‚Ä¢ "ÂØπÊØîÂàÜÊûêËøô‰∫õÊñá‰ª∂ÁöÑÁªüËÆ°‰ø°ÊÅØ"
‚Ä¢ "Ê£ÄÊµãÊâÄÊúâÊñá‰ª∂‰∏≠ÁöÑÂºÇÂ∏∏ÂÄº"
‚Ä¢ "ÂàÜÊûêÊñá‰ª∂Èó¥ÁöÑÊï∞ÊçÆÂ∑ÆÂºÇ"
‚Ä¢ "ÁîüÊàêÁªºÂêàÊï∞ÊçÆÊä•Âëä"

ÊàëÈöèÊó∂ÂáÜÂ§á‰∏∫ÊÇ®Êèê‰æõÊõ¥ËØ¶ÁªÜÁöÑÊâπÈáèÂàÜÊûêÔºÅ""",
            
            'code': f'''import pandas as pd
import numpy as np

# ÊâπÈáèËØªÂèñExcelÊñá‰ª∂
file_list = {[info['original_name'] for info in file_infos]}
file_data = {{}}

for file in file_list:
    df = pd.read_excel(file)
    file_data[file] = df
    print(f"Êñá‰ª∂ {{file}}: {{df.shape[0]}} Ë°å x {{df.shape[1]}} Âàó")

# ÊòæÁ§∫ÊâÄÊúâÊñá‰ª∂ÁöÑÂü∫Êú¨‰ø°ÊÅØ
print(f"\\nÂÖ±Âä†ËΩΩ {{len(file_list)}} ‰∏™Êñá‰ª∂")
print(f"ÊÄªÊï∞ÊçÆÈáèÈ¢Ñ‰º∞: {{sum([df.shape[0] for df in file_data.values()]):,}} Ë°å")

# ÂêàÂπ∂ÊâÄÊúâÊï∞ÊçÆÔºàÂ¶ÇÊûúÁªìÊûÑ‰∏ÄËá¥Ôºâ
try:
    combined_df = pd.concat([df.assign(source=name) for name, df in file_data.items()], ignore_index=True)
    print(f"\\nÊï∞ÊçÆÂêàÂπ∂ÊàêÂäü: {{combined_df.shape}}")
    print(combined_df.head())
except Exception as e:
    print(f"\\nÊï∞ÊçÆÂêàÂπ∂Â§±Ë¥•: {{e}}")
    print("Êñá‰ª∂ÁªìÊûÑÂèØËÉΩ‰∏ç‰∏ÄËá¥ÔºåÈúÄË¶ÅÂçïÁã¨ÂàÜÊûê")'''
        }


def generate_mock_response(query, file_info):
    """Generate mock responses for testing when agent system is not available."""
    query_lower = query.lower()
    
    if 'ÁªüËÆ°' in query_lower or 'Âü∫Êú¨‰ø°ÊÅØ' in query_lower:
        return {
            'analysis': f"""üìä Êï∞ÊçÆÁªüËÆ°ÂàÜÊûêÁªìÊûú

Âü∫‰∫éÊÇ®‰∏ä‰º†ÁöÑÊñá‰ª∂ "{file_info['original_name']}"Ôºö

üìã Êï∞ÊçÆÊ¶ÇËßàÔºö
‚Ä¢ ÊÄªË°åÊï∞: 1,234 Ë°å
‚Ä¢ ÊÄªÂàóÊï∞: 8 Âàó
‚Ä¢ Êñá‰ª∂Â§ßÂ∞è: {file_info['size']} bytes
‚Ä¢ Êï∞ÊçÆÁ±ªÂûã: 4‰∏™Êï∞ÂÄºÂàó, 3‰∏™ÊñáÊú¨Âàó, 1‰∏™Êó•ÊúüÂàó

üìà Êï∞ÂÄºÂàóÁªüËÆ°Ôºö
‚Ä¢ ÈîÄÂîÆÈ¢ù: Âπ≥ÂùáÂÄº ¬•8,547, ÊúÄÂ§ßÂÄº ¬•25,000, ÊúÄÂ∞èÂÄº ¬•1,200
‚Ä¢ Êï∞Èáè: Âπ≥ÂùáÂÄº 15.6, ÊúÄÂ§ßÂÄº 100, ÊúÄÂ∞èÂÄº 1
‚Ä¢ Âà©Ê∂¶Áéá: Âπ≥ÂùáÂÄº 23.4%, Ê†áÂáÜÂ∑Æ 8.9%

üîç Êï∞ÊçÆË¥®ÈáèÔºö
‚Ä¢ ÂÆåÊï¥ÊÄß: 98.7% (16‰∏™Áº∫Â§±ÂÄº)
‚Ä¢ ÈáçÂ§çËÆ∞ÂΩï: 3Êù°
‚Ä¢ ÂºÇÂ∏∏ÂÄº: Âú®ÈîÄÂîÆÈ¢ùÂàóÂèëÁé∞5‰∏™ÊΩúÂú®ÂºÇÂ∏∏ÂÄº

üí° ÂÖ≥ÈîÆÊ¥ûÂØüÔºö
1. ÁîµÂ≠ê‰∫ßÂìÅÁ±ªÂà´ÈîÄÂîÆÈ¢ùÂç†ÊÄªÈîÄÂîÆÈ¢ùÁöÑ68%
2. Q4ÈîÄÂîÆÈ¢ùÊØîQ3Â¢ûÈïø‰∫Ü15.3%
3. Âë®‰∫îÂíåÂë®ÂÖ≠ÊòØÈîÄÂîÆÈ´òÂ≥∞Êúü""",
            
            'code': f'''import pandas as pd
import numpy as np

# ËØªÂèñExcelÊñá‰ª∂
df = pd.read_excel('{file_info['original_name']}')

# Âü∫Êú¨ÁªüËÆ°‰ø°ÊÅØ
print("Êï∞ÊçÆÂΩ¢Áä∂:", df.shape)
print("Êï∞ÊçÆÁ±ªÂûã:")
print(df.dtypes)

# Êï∞ÂÄºÂàóÁªüËÆ°
numeric_cols = df.select_dtypes(include=[np.number]).columns
print("Êï∞ÂÄºÂàóÁªüËÆ°:")
print(df[numeric_cols].describe())

# Áº∫Â§±ÂÄºÊ£ÄÊü•
print("Áº∫Â§±ÂÄºÁªüËÆ°:")
print(df.isnull().sum())'''
        }
        
    elif 'ÂºÇÂ∏∏' in query_lower or 'Á¶ªÁæ§ÁÇπ' in query_lower:
        return {
            'analysis': """üîç ÂºÇÂ∏∏ÂÄºÊ£ÄÊµãÁªìÊûú

‰ΩøÁî®IQRÊñπÊ≥ïÂíåZ-ScoreÊñπÊ≥ïÊ£ÄÊµãÂºÇÂ∏∏ÂÄºÔºö

üö® ÂèëÁé∞ÁöÑÂºÇÂ∏∏ÂÄºÔºö
‚Ä¢ ÈîÄÂîÆÈ¢ùÂàó: 5‰∏™ÂºÇÂ∏∏ÂÄº
  - Á¨¨67Ë°å: ¬•87,500 (Z-score: 4.2)
  - Á¨¨156Ë°å: ¬•0 (ÂèØËÉΩÂΩïÂÖ•ÈîôËØØ)
  - Á¨¨234Ë°å: ¬•125,000 (Z-score: 6.8)

‚Ä¢ Êï∞ÈáèÂàó: 2‰∏™ÂºÇÂ∏∏ÂÄº
  - Á¨¨123Ë°å: 999 (ÊâπÂèëËÆ¢Âçï)
  - Á¨¨567Ë°å: 0 (Èõ∂Êï∞ÈáèÂºÇÂ∏∏)

üí° Â§ÑÁêÜÂª∫ËÆÆÔºö
1. È™åËØÅÊûÅÂÄºÁöÑÁúüÂÆûÊÄß
2. Ê£ÄÊü•Èõ∂ÂÄºÂíåË¥üÂÄº
3. ËÄÉËôë‰∏öÂä°ËßÑÂàôËøõË°åÊ∏ÖÊ¥ó""",
            
            'code': '''import pandas as pd
import numpy as np
from scipy import stats

def detect_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] < lower_bound) | (df[column] > upper_bound)]

# Ê£ÄÊµãÂºÇÂ∏∏ÂÄº
outliers = detect_outliers_iqr(df, 'sales_amount')
print(f"ÂèëÁé∞ {len(outliers)} ‰∏™ÂºÇÂ∏∏ÂÄº")'''
        }
        
    elif 'Áõ∏ÂÖ≥' in query_lower or 'ÂÖ≥ËÅî' in query_lower:
        return {
            'analysis': """üîó Áõ∏ÂÖ≥ÊÄßÂàÜÊûêÁªìÊûú

Êï∞ÊçÆÁõ∏ÂÖ≥ÊÄßÂàÜÊûêÊòæÁ§∫Ôºö

üìà Âº∫Ê≠£Áõ∏ÂÖ≥ (r > 0.7):
‚Ä¢ ÈîÄÂîÆÈ¢ù ‚Üî Âà©Ê∂¶: r = 0.89
‚Ä¢ ‰ª∑Ê†º ‚Üî Âà©Ê∂¶Áéá: r = 0.76
‚Ä¢ ÂπøÂëäÊäïÂÖ• ‚Üî ÈîÄÂîÆÈ¢ù: r = 0.72

üìâ Ë¥üÁõ∏ÂÖ≥ (r < -0.3):
‚Ä¢ ÊäòÊâ£Áéá ‚Üî Âà©Ê∂¶Áéá: r = -0.65
‚Ä¢ Â∫ìÂ≠òÈáè ‚Üî ÈîÄÂîÆÈÄüÂ∫¶: r = -0.45

üí° ‰∏öÂä°Ê¥ûÂØüÔºö
1. ÈîÄÂîÆÈ¢ù‰∏éÂà©Ê∂¶È´òÂ∫¶Áõ∏ÂÖ≥ÔºåÂÆö‰ª∑Á≠ñÁï•Á®≥ÂÆö
2. ÂπøÂëäÊäïÂÖ•ROIËâØÂ•Ω
3. ÊäòÊâ£Á≠ñÁï•ÈúÄË¶Å‰ºòÂåñ""",
            
            'code': '''import seaborn as sns
import matplotlib.pyplot as plt

# ËÆ°ÁÆóÁõ∏ÂÖ≥ÊÄßÁü©Èòµ
numeric_cols = df.select_dtypes(include=[np.number]).columns
correlation_matrix = df[numeric_cols].corr()

# ÂàõÂª∫ÁÉ≠ÂäõÂõæ
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Êï∞ÊçÆÁõ∏ÂÖ≥ÊÄßÁÉ≠ÂäõÂõæ')
plt.show()'''
        }
        
    elif 'ÂõæË°®' in query_lower or 'ÂèØËßÜÂåñ' in query_lower:
        return {
            'analysis': """üìä Êï∞ÊçÆÂèØËßÜÂåñÂàÜÊûê

Â∑≤‰∏∫ÊÇ®ÁîüÊàêÂ§öÁßçÂèØËßÜÂåñÂõæË°®Ôºö

üé® ÁîüÊàêÁöÑÂõæË°®Ôºö
1. üìà ÈîÄÂîÆË∂ãÂäøÁ∫øÂõæ - ÊòæÁ§∫ÊúàÂ∫¶ÂèòÂåñ
2. üìä ‰∫ßÂìÅÁ±ªÂà´È•ºÂõæ - Á±ªÂà´ÂàÜÂ∏É
3. üìã ÈîÄÂîÆÈ¢ùÂàÜÂ∏ÉÁõ¥ÊñπÂõæ - ÈáëÈ¢ùÂàÜÂ∏É
4. üî• Áõ∏ÂÖ≥ÊÄßÁÉ≠ÂäõÂõæ - ÂèòÈáèÂÖ≥Á≥ª

üí° ÂèØËßÜÂåñÊ¥ûÂØüÔºö
‚Ä¢ ÊòéÊòæÁöÑÂ≠£ËäÇÊÄßÈîÄÂîÆÊ®°Âºè
‚Ä¢ ‰∫ßÂìÅÁ±ªÂà´ÂàÜÂ∏É‰∏çÂùáÂåÄ
‚Ä¢ Â§ßÈÉ®ÂàÜ‰∏∫‰∏≠Â∞èÈ¢ùËÆ¢Âçï
‚Ä¢ ÂèòÈáèÈó¥Â≠òÂú®ÊúâË∂£ÂÖ≥ËÅî

ÊâÄÊúâÂõæË°®Â∑≤ÁîüÊàêÔºåÂèØÁî®‰∫éÊä•ÂëäÂ±ïÁ§∫„ÄÇ""",
            
            'code': '''import matplotlib.pyplot as plt
import seaborn as sns

# ËÆæÁΩÆ‰∏≠ÊñáÂ≠ó‰Ωì
plt.rcParams['font.sans-serif'] = ['SimHei']

# ÂàõÂª∫Â≠êÂõæ
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# 1. ÈîÄÂîÆË∂ãÂäø
monthly_sales = df.groupby('month')['sales'].sum()
axes[0,0].plot(monthly_sales.index, monthly_sales.values, marker='o')
axes[0,0].set_title('ÊúàÂ∫¶ÈîÄÂîÆË∂ãÂäø')

# 2. Á±ªÂà´ÂàÜÂ∏É
category_counts = df['category'].value_counts()
axes[0,1].pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%')
axes[0,1].set_title('‰∫ßÂìÅÁ±ªÂà´ÂàÜÂ∏É')

# 3. ÈîÄÂîÆÈ¢ùÂàÜÂ∏É
axes[1,0].hist(df['sales_amount'], bins=30, alpha=0.7)
axes[1,0].set_title('ÈîÄÂîÆÈ¢ùÂàÜÂ∏É')

# 4. Áõ∏ÂÖ≥ÊÄßÁÉ≠ÂäõÂõæ
corr = df.select_dtypes(include=[np.number]).corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', ax=axes[1,1])
axes[1,1].set_title('Áõ∏ÂÖ≥ÊÄßÁÉ≠ÂäõÂõæ')

plt.tight_layout()
plt.show()'''
        }
    
    else:
        return {
            'analysis': f"""ü§ñ AIÂàÜÊûêÂõûÂ§ç

ÈíàÂØπÊÇ®ÁöÑÈóÆÈ¢òÔºö"{query}"

Âü∫‰∫é‰∏ä‰º†ÁöÑÊñá‰ª∂ "{file_info['original_name']}"ÔºåÊàëÊèê‰æõ‰ª•‰∏ãÂàÜÊûêÔºö

üìä Êï∞ÊçÆÊ¶ÇÂÜµÔºö
‚Ä¢ Êñá‰ª∂Â§ßÂ∞è: {file_info['size']} bytes
‚Ä¢ ‰∏ä‰º†Êó∂Èó¥: {file_info['upload_time']}
‚Ä¢ Â§ÑÁêÜÁä∂ÊÄÅ: {"Â∑≤Â§ÑÁêÜ" if file_info.get('processed') else "ÂæÖÂ§ÑÁêÜ"}

üîç Âª∫ËÆÆÁöÑÂàÜÊûêÊñπÂêëÔºö
1. Âü∫Êú¨ÁªüËÆ°‰ø°ÊÅØÂàÜÊûê
2. Êï∞ÊçÆË¥®ÈáèÊ£ÄÊü•
3. ÂºÇÂ∏∏ÂÄºÊ£ÄÊµã
4. Áõ∏ÂÖ≥ÊÄßÂàÜÊûê
5. Ë∂ãÂäøÂàÜÊûê
6. ÂèØËßÜÂåñÂ±ïÁ§∫

üí° ÊÇ®ÂèØ‰ª•Â∞ùËØï‰ª•‰∏ãÂÖ∑‰ΩìÈóÆÈ¢òÔºö
‚Ä¢ "ÂàÜÊûêÂü∫Êú¨ÁªüËÆ°‰ø°ÊÅØ"
‚Ä¢ "Ê£ÄÊµãÂºÇÂ∏∏ÂÄºÂíåÁ¶ªÁæ§ÁÇπ"
‚Ä¢ "ÂàÜÊûêÊï∞ÊçÆÁõ∏ÂÖ≥ÊÄß"
‚Ä¢ "ÁîüÊàêÂèØËßÜÂåñÂõæË°®"

ÊàëÈöèÊó∂ÂáÜÂ§á‰∏∫ÊÇ®Êèê‰æõÊõ¥ËØ¶ÁªÜÁöÑÂàÜÊûêÔºÅ""",
            
            'code': f'''import pandas as pd
import numpy as np

# ËØªÂèñExcelÊñá‰ª∂
df = pd.read_excel('{file_info['original_name']}')

# Âü∫Êú¨‰ø°ÊÅØ
print("Êñá‰ª∂Âü∫Êú¨‰ø°ÊÅØ:")
print(f"Ë°åÊï∞: {{df.shape[0]}}")
print(f"ÂàóÊï∞: {{df.shape[1]}}")

# Êï∞ÊçÆÈ¢ÑËßà
print("Êï∞ÊçÆÈ¢ÑËßà:")
print(df.head())

# Êï∞ÊçÆÁ±ªÂûã
print("Êï∞ÊçÆÁ±ªÂûã:")
print(df.dtypes)'''
        }


@app.route('/api/files')
def list_files():
    """List all uploaded files."""
    files = []
    for file_id, file_info in uploaded_files.items():
        files.append({
            'file_id': file_id,
            'filename': file_info['original_name'],
            'size': file_info['size'],
            'upload_time': file_info['upload_time'],
            'processed': file_info['processed']
        })
    
    return jsonify({'files': files})


@app.route('/api/files/<file_id>', methods=['DELETE'])
@log_request_response
def delete_file(file_id):
    """Delete uploaded file."""
    try:
        if file_id not in uploaded_files:
            return jsonify({'error': 'File not found'}), 404
        
        file_info = uploaded_files[file_id]
        
        # Delete physical file
        if os.path.exists(file_info['file_path']):
            os.remove(file_info['file_path'])
        
        # Remove from memory
        del uploaded_files[file_id]
        
        return jsonify({'success': True, 'message': 'File deleted successfully'})
        
    except Exception as e:
        return jsonify({'error': f'Delete failed: {str(e)}'}), 500


@app.route('/api/preview/<file_id>', methods=['GET'])
@log_request_response
def preview_file(file_id):
    """Preview uploaded Excel file as HTML."""
    try:
        if file_id not in uploaded_files:
            return jsonify({'error': 'File not found'}), 404
        
        file_info = uploaded_files[file_id]
        file_path = file_info['file_path']
        
        # Get query parameters
        sheet_name = request.args.get('sheet')
        max_rows = min(int(request.args.get('max_rows', 100)), 1000)  # Cap at 1000 rows
        max_cols = min(int(request.args.get('max_cols', 20)), 50)    # Cap at 50 columns
        
        # Convert Excel to HTML
        result = excel_to_html(
            file_path=file_path,
            sheet_name=sheet_name,
            max_rows=max_rows,
            max_cols=max_cols,
            include_styling=True
        )
        
        if result['success']:
            return jsonify({
                'success': True,
                'file_id': file_id,
                'html': result['html'],
                'metadata': result['metadata'],
                'preview_data': result['preview_data']
            })
        else:
            return jsonify({
                'success': False,
                'error': result['error'],
                'html': result['html']
            }), 500
        
    except Exception as e:
        return jsonify({
            'success': False, 
            'error': f'Preview failed: {str(e)}',
            'html': f'<div class="error">Preview error: {str(e)}</div>'
        }), 500


@app.route('/api/preview/<file_id>/sheets', methods=['GET'])
@log_request_response
def get_file_sheets(file_id):
    """Get list of sheets in the Excel file."""
    try:
        if file_id not in uploaded_files:
            return jsonify({'error': 'File not found'}), 404
        
        file_info = uploaded_files[file_id]
        file_path = file_info['file_path']
        
        # Get Excel file information
        result = get_excel_info(file_path)
        
        if result['success']:
            return jsonify({
                'success': True,
                'file_id': file_id,
                'info': result['info']
            })
        else:
            return jsonify({
                'success': False,
                'error': result['error']
            }), 500
        
    except Exception as e:
        return jsonify({
            'success': False, 
            'error': f'Failed to get sheet info: {str(e)}'
        }), 500


@app.route('/api/preview/<file_id>/sheet/<sheet_name>', methods=['GET'])
@log_request_response  
def preview_specific_sheet(file_id, sheet_name):
    """Preview specific sheet of Excel file."""
    try:
        if file_id not in uploaded_files:
            return jsonify({'error': 'File not found'}), 404
        
        file_info = uploaded_files[file_id]
        file_path = file_info['file_path']
        
        # Get range parameters
        start_row = int(request.args.get('start_row', 0))
        end_row = request.args.get('end_row')
        if end_row:
            end_row = int(end_row)
        start_col = int(request.args.get('start_col', 0))  
        end_col = request.args.get('end_col')
        if end_col:
            end_col = int(end_col)
        
        # Convert specific sheet range to HTML
        result = excel_sheet_to_html(
            file_path=file_path,
            sheet_name=sheet_name,
            start_row=start_row,
            end_row=end_row,
            start_col=start_col,
            end_col=end_col
        )
        
        if result['success']:
            return jsonify({
                'success': True,
                'file_id': file_id,
                'sheet_name': sheet_name,
                'html': result['html'],
                'metadata': result['metadata']
            })
        else:
            return jsonify({
                'success': False,
                'error': result['error'],
                'html': result['html']
            }), 500
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'Sheet preview failed: {str(e)}',
            'html': f'<div class="error">Sheet preview error: {str(e)}</div>'
        }), 500


# ================================
# LLM Configuration API Endpoints
# ================================

@app.route('/api/llm/config', methods=['GET'])
@log_request_response
def get_llm_config():
    """Get current LLM configuration."""
    try:
        current_params = config.get_llm_params()
        available_models = config.get_available_models()
        
        return jsonify({
            'success': True,
            'current_params': current_params,
            'available_models': available_models,
            'parameter_limits': {
                'temperature': {'min': 0.0, 'max': 2.0, 'step': 0.1},
                'max_tokens': {'min': 1, 'max': 4000, 'step': 1},
                'top_p': {'min': 0.0, 'max': 1.0, 'step': 0.01},
                'frequency_penalty': {'min': -2.0, 'max': 2.0, 'step': 0.1},
                'presence_penalty': {'min': -2.0, 'max': 2.0, 'step': 0.1}
            }
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'Failed to get LLM config: {str(e)}'
        }), 500


@app.route('/api/llm/config', methods=['POST'])
@log_request_response
def update_llm_config():
    """Update LLM configuration."""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'error': 'No configuration data provided'}), 400
        
        # Extract valid parameters
        valid_params = {}
        for key in ['model', 'temperature', 'max_tokens', 'top_p', 'frequency_penalty', 'presence_penalty', 'stream']:
            if key in data:
                valid_params[key] = data[key]
        
        if not valid_params:
            return jsonify({'error': 'No valid parameters provided'}), 400
        
        # Update configuration
        config.update_llm_params(**valid_params)
        
        # Log the configuration change
        api_logger.info(f"üîß [Config] LLM parameters updated: {valid_params}")
        
        return jsonify({
            'success': True,
            'message': 'LLM configuration updated successfully',
            'updated_params': valid_params,
            'current_params': config.get_llm_params()
        })
        
    except ValueError as e:
        return jsonify({
            'success': False,
            'error': f'Invalid parameter value: {str(e)}'
        }), 400
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'Failed to update LLM config: {str(e)}'
        }), 500


@app.route('/api/llm/config/reset', methods=['POST'])
@log_request_response
def reset_llm_config():
    """Reset LLM configuration to default values."""
    try:
        config.reset_llm_params_to_default()
        
        api_logger.info("üîß [Config] LLM parameters reset to default values")
        
        return jsonify({
            'success': True,
            'message': 'LLM configuration reset to default values',
            'current_params': config.get_llm_params()
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'Failed to reset LLM config: {str(e)}'
        }), 500


@app.route('/api/llm/models', methods=['GET'])
@log_request_response
def get_available_models():
    """Get list of available models."""
    try:
        models = config.get_available_models()
        return jsonify({
            'success': True,
            'models': models
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'Failed to get models: {str(e)}'
        }), 500


@app.route('/api/llm/presets', methods=['GET'])
@log_request_response
def get_llm_presets():
    """Get LLM configuration presets."""
    presets = {
        'creative': {
            'name': 'ÂàõÊÑèÊ®°Âºè',
            'description': 'ÈÄÇÂêàÂàõÈÄ†ÊÄß‰ªªÂä°ÔºåÊõ¥Â§öÂèòÂåñÂíåÂàõÊñ∞',
            'params': {
                'temperature': 1.0,
                'top_p': 0.9,
                'frequency_penalty': 0.5,
                'presence_penalty': 0.3,
                'max_tokens': 2000
            }
        },
        'balanced': {
            'name': 'Âπ≥Ë°°Ê®°Âºè',
            'description': 'ÈÄÇÂêà‰∏ÄËà¨‰ªªÂä°ÔºåÂπ≥Ë°°ÂàõÈÄ†ÊÄßÂíåÂáÜÁ°ÆÊÄß',
            'params': {
                'temperature': 0.7,
                'top_p': 1.0,
                'frequency_penalty': 0.0,
                'presence_penalty': 0.0,
                'max_tokens': 1500
            }
        },
        'precise': {
            'name': 'Á≤æÁ°ÆÊ®°Âºè',
            'description': 'ÈÄÇÂêàÈúÄË¶ÅÂáÜÁ°ÆÁ≠îÊ°àÁöÑ‰ªªÂä°ÔºåÊõ¥Á°ÆÂÆöÊÄß',
            'params': {
                'temperature': 0.3,
                'top_p': 0.8,
                'frequency_penalty': 0.0,
                'presence_penalty': 0.0,
                'max_tokens': 1000
            }
        },
        'analytical': {
            'name': 'ÂàÜÊûêÊ®°Âºè',
            'description': 'ÈÄÇÂêàÊï∞ÊçÆÂàÜÊûêÂíåÈÄªËæëÊé®ÁêÜ',
            'params': {
                'temperature': 0.1,
                'top_p': 0.95,
                'frequency_penalty': 0.2,
                'presence_penalty': 0.1,
                'max_tokens': 3000
            }
        }
    }
    
    return jsonify({
        'success': True,
        'presets': presets
    })


@app.route('/api/llm/presets/<preset_name>', methods=['POST'])
@log_request_response
def apply_llm_preset(preset_name):
    """Apply a specific LLM configuration preset."""
    try:
        # Get presets
        presets_response = get_llm_presets()
        presets_data = presets_response.get_json()
        
        if not presets_data['success']:
            return jsonify({'error': 'Failed to get presets'}), 500
        
        presets = presets_data['presets']
        
        if preset_name not in presets:
            return jsonify({'error': f'Preset "{preset_name}" not found'}), 404
        
        # Apply preset parameters
        preset_params = presets[preset_name]['params']
        config.update_llm_params(**preset_params)
        
        api_logger.info(f"üîß [Config] Applied LLM preset '{preset_name}': {preset_params}")
        
        return jsonify({
            'success': True,
            'message': f'Applied preset: {presets[preset_name]["name"]}',
            'preset': presets[preset_name],
            'current_params': config.get_llm_params()
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'Failed to apply preset: {str(e)}'
        }), 500


if __name__ == '__main__':
    # Initialize agent system
    if AGENT_AVAILABLE:
        print("Attempting to initialize agent system...")
        try:
            success = asyncio.run(initialize_agent_system())
            if success:
                print("‚úÖ Agent system initialized successfully!")
            else:
                print("‚ö†Ô∏è  Agent system initialization failed, running in mock mode")
        except Exception as e:
            print(f"‚ö†Ô∏è  Agent system initialization error: {e}")
            print("Running in mock mode...")
    else:
        print("‚ö†Ô∏è  Agent system not available, running in mock mode")
    
    print(f"Upload folder: {app.config['UPLOAD_FOLDER']}")
    print("Starting Flask server...")
    app.run(debug=True, host='0.0.0.0', port=5000)