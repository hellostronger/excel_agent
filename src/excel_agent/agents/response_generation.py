"""Response Generation Agent - Synthesizes all agent results and generates user-friendly responses."""

import json
from typing import Any, Dict, List, Optional
from datetime import datetime

from .base import BaseAgent
from ..models.base import AgentRequest, AgentResponse, AgentStatus
from ..models.agents import ResponseGenerationRequest, ResponseGenerationResponse
from ..utils.siliconflow_client import SiliconFlowClient


class ResponseGenerationAgent(BaseAgent):
    """Agent responsible for generating comprehensive responses based on all agent results."""
    
    def __init__(self):
        super().__init__(
            name="ResponseGenerationAgent",
            description="Synthesizes all agent results and generates natural language responses for users"
        )
    
    async def process(self, request: AgentRequest) -> AgentResponse:
        """Process response generation request."""
        if not isinstance(request, ResponseGenerationRequest):
            return self.create_error_response(
                request,
                f"Invalid request type. Expected ResponseGenerationRequest, got {type(request)}"
            )
        
        try:
            self.logger.info(f"Generating response for user query: {request.user_query[:100]}...")
            
            # Generate response based on workflow results
            response_content = await self._generate_comprehensive_response(
                user_query=request.user_query,
                workflow_results=request.workflow_results,
                file_info=request.file_info,
                context=request.context or {}
            )
            
            # Format the response
            formatted_response = await self._format_response(
                response_content=response_content,
                workflow_results=request.workflow_results,
                user_query=request.user_query
            )
            
            self.logger.info(f"Response generation completed successfully")
            
            return ResponseGenerationResponse(
                agent_id=self.name,
                request_id=request.request_id,
                status=AgentStatus.SUCCESS,
                response=formatted_response['main_response'],
                summary=formatted_response['summary'],
                recommendations=formatted_response['recommendations'],
                technical_details=formatted_response['technical_details'],
                result={
                    'response_generated': True,
                    'response_length': len(formatted_response['main_response']),
                    'includes_recommendations': len(formatted_response['recommendations']) > 0,
                    'includes_technical_details': bool(formatted_response['technical_details'])
                }
            )
            
        except Exception as e:
            self.logger.error(f"Error during response generation: {e}")
            return self.create_error_response(request, str(e))
    
    async def _generate_comprehensive_response(
        self,
        user_query: str,
        workflow_results: Dict[str, Any],
        file_info: Optional[Dict[str, Any]] = None,
        context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """Generate a comprehensive response based on all available data."""
        
        # Prepare context for the LLM
        serializable_workflow = self._make_json_serializable(workflow_results)
        serializable_file_info = self._make_json_serializable(file_info or {})
        serializable_context = self._make_json_serializable(context or {})
        
        # Build the prompt for response generation
        response_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Excelæ•°æ®åˆ†æåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ä¸ºç”¨æˆ·ç”Ÿæˆä¸€ä¸ªå…¨é¢ã€å‡†ç¡®ã€æ˜“æ‡‚çš„å›ç­”ã€‚

ç”¨æˆ·é—®é¢˜: "{user_query}"

æ–‡ä»¶ä¿¡æ¯:
{json.dumps(serializable_file_info, indent=2, ensure_ascii=False)}

å·¥ä½œæµæ‰§è¡Œç»“æœ:
{json.dumps(serializable_workflow, indent=2, ensure_ascii=False)}

ä¸Šä¸‹æ–‡ä¿¡æ¯:
{json.dumps(serializable_context, indent=2, ensure_ascii=False)}

è¯·ç”Ÿæˆä¸€ä¸ªåŒ…å«ä»¥ä¸‹å†…å®¹çš„å“åº”:

1. **ä¸»è¦å›ç­”**: ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œä½¿ç”¨è‡ªç„¶ã€å‹å¥½çš„è¯­è¨€
2. **æ•°æ®æ´å¯Ÿ**: åŸºäºåˆ†æç»“æœæä¾›æœ‰ä»·å€¼çš„æ•°æ®æ´å¯Ÿ
3. **å…·ä½“æ•°å€¼**: åŒ…å«ç›¸å…³çš„ç»Ÿè®¡æ•°æ®ã€è®¡ç®—ç»“æœç­‰
4. **å»ºè®®**: åŸºäºåˆ†æç»“æœç»™å‡ºå®ç”¨çš„å»ºè®®
5. **æŠ€æœ¯è¯´æ˜**: ç®€è¦è¯´æ˜ä½¿ç”¨çš„åˆ†ææ–¹æ³•ï¼ˆå¦‚æœé€‚ç”¨ï¼‰

è¦æ±‚:
- ä½¿ç”¨ä¸­æ–‡å›ç­”
- è¯­è¨€è‡ªç„¶æµç•…ï¼Œé¿å…è¿‡äºæŠ€æœ¯åŒ–
- çªå‡ºå…³é”®æ•°æ®å’Œå‘ç°
- å¦‚æœæœ‰é”™è¯¯æˆ–è­¦å‘Šï¼Œè¦æ˜ç¡®è¯´æ˜
- å¦‚æœåˆ†æä¸å®Œæ•´ï¼Œè¦è¯šå®è¯´æ˜é™åˆ¶

è¯·ä»¥JSONæ ¼å¼è¿”å›:
{{
    "main_response": "ä¸»è¦å›ç­”å†…å®¹",
    "data_insights": ["æ´å¯Ÿ1", "æ´å¯Ÿ2"],
    "key_numbers": {{"æŒ‡æ ‡1": "æ•°å€¼1", "æŒ‡æ ‡2": "æ•°å€¼2"}},
    "recommendations": ["å»ºè®®1", "å»ºè®®2"],
    "technical_notes": "æŠ€æœ¯è¯´æ˜",
    "confidence_level": "high|medium|low",
    "limitations": ["é™åˆ¶1", "é™åˆ¶2"]
}}
"""

        try:
            import time
            from ..utils.logging import get_logger
            
            logger = get_logger(__name__)
            request_id = context.get('request_id', 'response_generation')
            
            # Log request start
            start_time = time.time()
            logger.info(f"ğŸ¤– [ResponseGeneration] Starting LLM request {request_id}")
            
            async with SiliconFlowClient() as client:
                try:
                    response = await client.chat_completion(
                        messages=[{"role": "user", "content": response_prompt}],
                        temperature=0.7,
                        request_id=request_id
                    )
                    
                    # Log successful completion
                    end_time = time.time()
                    duration = end_time - start_time
                    logger.info(f"ğŸ¤– [ResponseGeneration] LLM request {request_id} completed in {duration:.2f}s")
                    
                except Exception as e:
                    # Log error
                    end_time = time.time()
                    duration = end_time - start_time
                    logger.error(f"âŒ [ResponseGeneration] LLM request {request_id} failed after {duration:.2f}s: {type(e).__name__}: {e}")
                    raise
            
            # Parse the response
            response_text = response['choices'][0]['message']['content']
            
            # Extract JSON from response if wrapped in code blocks
            if '```json' in response_text:
                json_start = response_text.find('```json') + 7
                json_end = response_text.find('```', json_start)
                response_text = response_text[json_start:json_end]
            
            response_data = json.loads(response_text.strip())
            
            return response_data
            
        except (json.JSONDecodeError, KeyError) as e:
            self.logger.warning(f"Failed to parse LLM response: {e}")
            # Fallback to simple response generation
            return self._generate_fallback_response(user_query, workflow_results)
    
    async def _format_response(
        self,
        response_content: Dict[str, Any],
        workflow_results: Dict[str, Any],
        user_query: str
    ) -> Dict[str, Any]:
        """Format the response for better presentation."""
        
        # Extract key components
        main_response = response_content.get('main_response', 'æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå®Œæ•´çš„å›ç­”ã€‚')
        data_insights = response_content.get('data_insights', [])
        key_numbers = response_content.get('key_numbers', {})
        recommendations = response_content.get('recommendations', [])
        technical_notes = response_content.get('technical_notes', '')
        confidence_level = response_content.get('confidence_level', 'medium')
        limitations = response_content.get('limitations', [])
        
        # Build formatted response
        formatted_main = self._build_formatted_main_response(
            main_response, data_insights, key_numbers, confidence_level
        )
        
        # Generate summary
        summary = self._generate_summary(workflow_results, key_numbers)
        
        # Format technical details
        technical_details = self._format_technical_details(
            workflow_results, technical_notes, limitations
        )
        
        return {
            'main_response': formatted_main,
            'summary': summary,
            'recommendations': recommendations,
            'technical_details': technical_details
        }
    
    def _build_formatted_main_response(
        self,
        main_response: str,
        data_insights: List[str],
        key_numbers: Dict[str, Any],
        confidence_level: str
    ) -> str:
        """Build a well-formatted main response."""
        
        formatted_parts = [main_response]
        
        # Add key numbers if available
        if key_numbers:
            formatted_parts.append("\nğŸ“Š **å…³é”®æ•°æ®:**")
            for key, value in key_numbers.items():
                formatted_parts.append(f"â€¢ {key}: {value}")
        
        # Add insights if available
        if data_insights:
            formatted_parts.append("\nğŸ’¡ **æ•°æ®æ´å¯Ÿ:**")
            for insight in data_insights:
                formatted_parts.append(f"â€¢ {insight}")
        
        # Add confidence indicator
        confidence_emoji = {
            'high': 'ğŸŸ¢',
            'medium': 'ğŸŸ¡', 
            'low': 'ğŸŸ '
        }
        emoji = confidence_emoji.get(confidence_level, 'ğŸŸ¡')
        confidence_text = {
            'high': 'é«˜åº¦å¯ä¿¡',
            'medium': 'ä¸­ç­‰å¯ä¿¡',
            'low': 'éœ€è¦è¿›ä¸€æ­¥éªŒè¯'
        }
        text = confidence_text.get(confidence_level, 'ä¸­ç­‰å¯ä¿¡')
        
        formatted_parts.append(f"\n{emoji} **å¯ä¿¡åº¦**: {text}")
        
        return '\n'.join(formatted_parts)
    
    def _generate_summary(
        self,
        workflow_results: Dict[str, Any],
        key_numbers: Dict[str, Any]
    ) -> str:
        """Generate a concise summary of the analysis."""
        
        summary_parts = []
        
        # Workflow status
        status = workflow_results.get('status', 'unknown')
        if status == 'success':
            summary_parts.append("âœ… æ•°æ®åˆ†ææˆåŠŸå®Œæˆ")
        else:
            summary_parts.append("âš ï¸ æ•°æ®åˆ†æéƒ¨åˆ†å®Œæˆ")
        
        # Processing info
        if workflow_results.get('file_processed'):
            processed_file = workflow_results.get('processed_file', 'æœªçŸ¥æ–‡ä»¶')
            summary_parts.append(f"ğŸ“ å·²å¤„ç†æ–‡ä»¶: {processed_file}")
        
        # Key metrics count
        if key_numbers:
            summary_parts.append(f"ğŸ“ˆ ç”Ÿæˆäº† {len(key_numbers)} é¡¹å…³é”®æŒ‡æ ‡")
        
        # Workflow steps
        steps = workflow_results.get('steps', [])
        if steps:
            completed_steps = [s for s in steps if s.get('status') == 'success']
            summary_parts.append(f"âš™ï¸ å®Œæˆäº† {len(completed_steps)}/{len(steps)} ä¸ªå¤„ç†æ­¥éª¤")
        
        return '\n'.join(summary_parts)
    
    def _format_technical_details(
        self,
        workflow_results: Dict[str, Any],
        technical_notes: str,
        limitations: List[str]
    ) -> Dict[str, Any]:
        """Format technical details for advanced users."""
        
        details = {}
        
        # Workflow information
        details['workflow_type'] = workflow_results.get('workflow_type', 'unknown')
        details['execution_time'] = workflow_results.get('execution_time', 0)
        
        # Processing steps
        steps = workflow_results.get('steps', [])
        details['processing_steps'] = [
            {
                'step': step.get('step', 'unknown'),
                'status': step.get('status', 'unknown')
            }
            for step in steps
        ]
        
        # Generated code
        if workflow_results.get('generated_code'):
            details['code_generated'] = True
            details['code_length'] = len(workflow_results['generated_code'])
        
        # Technical notes
        if technical_notes:
            details['technical_notes'] = technical_notes
        
        # Limitations
        if limitations:
            details['limitations'] = limitations
        
        # MCP usage
        details['mcp_enhanced'] = any(
            'mcp' in str(step).lower() 
            for step in steps
        )
        
        return details
    
    def _generate_fallback_response(
        self,
        user_query: str,
        workflow_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Generate a fallback response when LLM fails."""
        
        status = workflow_results.get('status', 'unknown')
        
        if status == 'success':
            main_response = f"å·²æˆåŠŸå¤„ç†æ‚¨çš„é—®é¢˜ï¼š{user_query}\n\nåŸºäºæ•°æ®åˆ†æç»“æœï¼Œæˆ‘å·²å®Œæˆç›¸å…³å¤„ç†ã€‚"
            
            # Add execution output if available
            if workflow_results.get('output'):
                main_response += f"\n\næ‰§è¡Œç»“æœï¼š\n{workflow_results['output']}"
                
        else:
            main_response = f"åœ¨å¤„ç†æ‚¨çš„é—®é¢˜æ—¶é‡åˆ°äº†ä¸€äº›å›°éš¾ï¼š{user_query}\n\nè¯·æ£€æŸ¥æ•°æ®æ ¼å¼æˆ–é‡æ–°è¡¨è¿°æ‚¨çš„é—®é¢˜ã€‚"
        
        return {
            'main_response': main_response,
            'data_insights': [],
            'key_numbers': {},
            'recommendations': ["å»ºè®®æ£€æŸ¥æ•°æ®æ ¼å¼", "å¦‚éœ€å¸®åŠ©è¯·é‡æ–°è¡¨è¿°é—®é¢˜"],
            'technical_notes': 'ä½¿ç”¨äº†å¤‡ç”¨å“åº”ç”Ÿæˆæ–¹å¼',
            'confidence_level': 'low',
            'limitations': ['LLMå“åº”ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–å›ç­”']
        }