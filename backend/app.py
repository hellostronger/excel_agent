"""Flask backend for Excel Intelligent Agent System with RAG capabilities."""

import os
import asyncio
import sys
import tempfile
import uuid
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List

from flask import Flask, request, jsonify, render_template, g, Response
from flask_cors import CORS
from werkzeug.utils import secure_filename
import logging
import time
import json

# Import our Excel to HTML utility
from utils.excel_to_html import excel_to_html, get_excel_sheets, excel_sheet_to_html, get_excel_info
from utils.file_manager import file_manager
from utils.relevance_matcher import relevance_matcher
from utils.progress_tracker import progress_tracker, ProgressUpdate

# Add src directory to Python path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

# Try to import the agent system with fallback
try:
    from excel_agent.core.orchestrator import Orchestrator
    from excel_agent.mcp.agent_configs import initialize_mcp_system, initialize_all_agent_mcp
    from excel_agent.utils.config import config
    AGENT_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Agent system not available: {e}")
    AGENT_AVAILABLE = False
    # Create a mock config for fallback
    class MockConfig:
        def get_llm_params(self):
            return {'model': 'mock', 'temperature': 0.7, 'max_tokens': None}
        def update_llm_params(self, **params):
            pass
        def get_available_models(self):
            return {'llm_models': ['mock-model']}
        def reset_llm_params_to_default(self):
            pass
    config = MockConfig()

app = Flask(__name__)
CORS(app)

# Configuration
app.config['MAX_CONTENT_LENGTH'] = 50 * 1024 * 1024  # 50MB max file size
app.config['UPLOAD_FOLDER'] = tempfile.mkdtemp()
app.config['JSON_AS_ASCII'] = False  # Á°Æ‰øùJSONÂìçÂ∫îÊîØÊåÅUTF-8ÁºñÁ†Å

# Setup detailed logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(name)s | %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('excel_agent_api.log')
    ]
)

api_logger = logging.getLogger('excel_agent.api')
logger = logging.getLogger('excel_agent.lightweight_processing')

# Á°Æ‰øùÊâÄÊúâÂìçÂ∫îÈÉΩ‰ΩøÁî®UTF-8ÁºñÁ†Å
@app.after_request
def after_request(response):
    """ËÆæÁΩÆÂìçÂ∫îÂ§¥Á°Æ‰øùUTF-8ÁºñÁ†Å"""
    if response.content_type.startswith('application/json'):
        response.content_type = 'application/json; charset=utf-8'
    response.headers['Content-Type'] = response.content_type
    return response


def log_request_response(f):
    """Decorator to log API requests and responses."""
    def decorated_function(*args, **kwargs):
        # Log request
        request_id = str(uuid.uuid4())[:8]
        g.request_id = request_id
        
        start_time = time.time()
        
        # Log request details
        api_logger.info(f"üì• [Request {request_id}] {request.method} {request.path}")
        api_logger.info(f"üì• [Request {request_id}] Remote: {request.remote_addr}")
        api_logger.info(f"üì• [Request {request_id}] User-Agent: {request.headers.get('User-Agent', 'N/A')}")
        
        # Log request body (if present and not too large)
        if request.is_json and request.content_length and request.content_length < 10000:
            try:
                request_data = request.get_json()
                # Mask sensitive data
                if isinstance(request_data, dict) and 'query' in request_data:
                    log_data = request_data.copy()
                    if len(log_data['query']) > 200:
                        log_data['query'] = log_data['query'][:200] + "... (truncated)"
                    api_logger.debug(f"üì• [Request {request_id}] Body: {json.dumps(log_data, ensure_ascii=False)}")
                else:
                    api_logger.debug(f"üì• [Request {request_id}] Body: {json.dumps(request_data, ensure_ascii=False)}")
            except Exception as e:
                api_logger.debug(f"üì• [Request {request_id}] Body: <unable to parse: {e}>")
        elif request.files:
            files_info = []
            for file_key, file_obj in request.files.items():
                files_info.append(f"{file_key}: {file_obj.filename} ({file_obj.content_length} bytes)")
            api_logger.info(f"üì• [Request {request_id}] Files: {', '.join(files_info)}")
        
        try:
            # Execute the function
            response = f(*args, **kwargs)
            
            # Log response
            end_time = time.time()
            response_time = end_time - start_time
            
            if hasattr(response, 'status_code'):
                status_code = response.status_code
                api_logger.info(f"üì§ [Response {request_id}] Status: {status_code}, Time: {response_time:.3f}s")
                
                # Log response body (if JSON and not too large)
                if hasattr(response, 'is_json') and response.is_json and response.content_length and response.content_length < 10000:
                    try:
                        response_data = response.get_json()
                        if isinstance(response_data, dict):
                            log_data = response_data.copy()
                            # Truncate long responses
                            if 'response' in log_data and len(str(log_data['response'])) > 500:
                                log_data['response'] = str(log_data['response'])[:500] + "... (truncated)"
                            if 'generated_code' in log_data and len(str(log_data['generated_code'])) > 500:
                                log_data['generated_code'] = str(log_data['generated_code'])[:500] + "... (truncated)"
                            api_logger.debug(f"üì§ [Response {request_id}] Body: {json.dumps(log_data, ensure_ascii=False)}")
                    except Exception as e:
                        api_logger.debug(f"üì§ [Response {request_id}] Body: <unable to parse: {e}>")
            else:
                api_logger.info(f"üì§ [Response {request_id}] Time: {response_time:.3f}s")
            
            return response
            
        except Exception as e:
            # Log error
            end_time = time.time()
            response_time = end_time - start_time
            api_logger.error(f"‚ùå [Error {request_id}] {str(e)}, Time: {response_time:.3f}s")
            raise
    
    decorated_function.__name__ = f.__name__
    return decorated_function

# Allowed file extensions
ALLOWED_EXTENSIONS = {'xlsx', 'xls', 'xlsm'}

# Global variables
orchestrator = None
uploaded_files = {}
mcp_initialized = False

# Progress tracking for Server-Sent Events
progress_connections = {}  # request_id -> list of connections

# Initialize file recovery at startup
def initialize_file_recovery():
    """Recover uploaded files from persistent storage."""
    global uploaded_files
    try:
        stored_files = file_manager.list_files()
        uploaded_files.update(stored_files)
        print(f"Recovered {len(stored_files)} files from persistent storage")
        
        # Clean up old files (older than 24 hours)
        cleaned_count = file_manager.cleanup_old_files(max_age_hours=24)
        if cleaned_count > 0:
            print(f"Cleaned up {cleaned_count} old files")
            
    except Exception as e:
        print(f"Error during file recovery: {e}")

# Recover files on startup
initialize_file_recovery()


# Progress tracking helper functions
def send_progress_to_client(progress_update: ProgressUpdate):
    """Send progress update to connected clients via SSE."""
    request_id = progress_update.request_id
    if request_id in progress_connections:
        # Convert to JSON
        data = {
            'type': 'progress',
            'request_id': progress_update.request_id,
            'current_step': progress_update.current_step,
            'current_step_name': progress_update.current_step_name,
            'progress_percent': progress_update.progress_percent,
            'status': progress_update.status,
            'message': progress_update.message,
            'agent': progress_update.agent,
            'timestamp': progress_update.timestamp,
            'details': progress_update.details,
            'all_steps': [
                {
                    'step_id': step.step_id,
                    'step_name': step.step_name,
                    'status': step.status,
                    'agent': step.agent,
                    'description': step.description,
                    'started_at': step.started_at,
                    'completed_at': step.completed_at,
                    'progress_percent': step.progress_percent,
                    'details': step.details
                }
                for step in progress_update.all_steps
            ]
        }
        
        # Format as Server-Sent Event
        sse_data = f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
        
        # Send to all connections for this request
        queues_to_remove = []
        for message_queue in progress_connections[request_id]:
            try:
                message_queue.put(sse_data)
            except Exception as e:
                api_logger.error(f"Failed to send progress to client: {e}")
                queues_to_remove.append(message_queue)
        
        # Remove failed connections
        for queue_obj in queues_to_remove:
            progress_connections[request_id].remove(queue_obj)
        
        # Clean up empty connection lists
        if not progress_connections[request_id]:
            del progress_connections[request_id]


def cleanup_progress_connections():
    """Periodically clean up old connections."""
    # This would be called by a background task
    pass


# Register progress callback
progress_tracker.add_progress_callback(send_progress_to_client)


def allowed_file(filename):
    """Check if file extension is allowed."""
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS


def get_file_info_safe(file_id: str):
    """
    Safely get file info, trying memory cache first, then persistent storage.
    
    Returns:
        file_info dict or None if not found
    """
    global uploaded_files
    
    # Check in-memory cache first
    if file_id in uploaded_files:
        return uploaded_files[file_id]
    
    # Try to recover from persistent storage
    file_info = file_manager.get_file_info(file_id)
    if file_info is not None:
        # Add to in-memory cache
        uploaded_files[file_id] = file_info
        return file_info
    
    return None


async def initialize_agent_system():
    """Initialize the agent system with MCP capabilities."""
    global orchestrator, mcp_initialized
    
    if not AGENT_AVAILABLE:
        return False
    
    try:
        print("Initializing Agent System...")
        
        # Initialize MCP system
        initialize_mcp_system()
        await initialize_all_agent_mcp()
        mcp_initialized = True
        
        # Initialize orchestrator
        orchestrator = Orchestrator()
        
        print("Agent System initialized successfully!")
        return True
        
    except Exception as e:
        print(f"Failed to initialize agent system: {e}")
        return False


def run_async(coro):
    """Run async function in sync context."""
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # If loop is already running, create a new task
            return loop.create_task(coro)
        else:
            return loop.run_until_complete(coro)
    except RuntimeError as e:
        # No event loop in current thread, create a new one
        if "There is no current event loop" in str(e):
            return asyncio.run(coro)
        # For other RuntimeErrors, try to create new event loop
        try:
            new_loop = asyncio.new_event_loop()
            asyncio.set_event_loop(new_loop)
            return new_loop.run_until_complete(coro)
        finally:
            new_loop.close()
            asyncio.set_event_loop(None)


@app.route('/')
def index():
    """Serve the main page."""
    return render_template('index.html')


@app.route('/chat')
def chat():
    """Serve the new chat interface."""
    return render_template('chat.html')


@app.route('/api/status')
@log_request_response
def get_status():
    """Get system status."""
    status = {
        'agent_available': AGENT_AVAILABLE,
        'mcp_initialized': mcp_initialized,
        'orchestrator_ready': orchestrator is not None,
        'upload_folder': app.config['UPLOAD_FOLDER'],
        'max_file_size': app.config['MAX_CONTENT_LENGTH'],
        'allowed_extensions': list(ALLOWED_EXTENSIONS)
    }
    
    if orchestrator and mcp_initialized:
        try:
            # Get MCP status if available
            mcp_status = asyncio.run(orchestrator.get_mcp_status())
            status['mcp_status'] = mcp_status
        except Exception as e:
            status['mcp_error'] = str(e)
    
    return jsonify(status)


@app.route('/api/progress/<request_id>')
def progress_stream(request_id):
    """Server-Sent Events stream for progress updates."""
    import queue
    import threading
    
    def event_generator():
        # Create a message queue for this connection
        message_queue = queue.Queue()
        connection_active = {'active': True}
        
        # Add this queue to progress connections
        if request_id not in progress_connections:
            progress_connections[request_id] = []
        progress_connections[request_id].append(message_queue)
        
        try:
            # Send initial connection message
            initial_message = {
                'type': 'connected',
                'request_id': request_id,
                'timestamp': datetime.now().isoformat(),
                'message': 'ËøûÊé•Â∑≤Âª∫Á´ãÔºåÁ≠âÂæÖËøõÂ∫¶Êõ¥Êñ∞...'
            }
            
            yield f"data: {json.dumps(initial_message, ensure_ascii=False)}\n\n"
            
            # Check for existing progress data
            existing_progress = progress_tracker.get_request_progress(request_id)
            if existing_progress:
                # Send current progress state
                progress_data = {
                    'type': 'progress',
                    'request_id': request_id,
                    'current_step': existing_progress['steps'][existing_progress['current_step_index']].step_id if existing_progress['steps'] else '',
                    'current_step_name': existing_progress['steps'][existing_progress['current_step_index']].step_name if existing_progress['steps'] else '',
                    'progress_percent': existing_progress['progress_percent'],
                    'status': existing_progress['status'],
                    'message': f"ÂΩìÂâçËøõÂ∫¶: {existing_progress['progress_percent']}%",
                    'agent': existing_progress['steps'][existing_progress['current_step_index']].agent if existing_progress['steps'] else '',
                    'timestamp': datetime.now().isoformat(),
                    'all_steps': [
                        {
                            'step_id': step.step_id,
                            'step_name': step.step_name,
                            'status': step.status,
                            'agent': step.agent,
                            'description': step.description,
                            'progress_percent': step.progress_percent
                        }
                        for step in existing_progress['steps']
                    ]
                }
                yield f"data: {json.dumps(progress_data, ensure_ascii=False)}\n\n"
            
            # Keep connection alive and check for messages
            last_heartbeat = time.time()
            
            while connection_active['active']:
                try:
                    # Check for new messages with timeout
                    try:
                        message_data = message_queue.get(timeout=5.0)
                        yield message_data
                        message_queue.task_done()
                    except queue.Empty:
                        # Send heartbeat if needed
                        current_time = time.time()
                        if current_time - last_heartbeat > 25:  # Every 25 seconds
                            heartbeat = {
                                'type': 'heartbeat',
                                'timestamp': datetime.now().isoformat()
                            }
                            yield f"data: {json.dumps(heartbeat, ensure_ascii=False)}\n\n"
                            last_heartbeat = current_time
                        
                        # Check if request is completed
                        current_progress = progress_tracker.get_request_progress(request_id)
                        if current_progress and current_progress['status'] in ['completed', 'failed']:
                            api_logger.info(f"Progress stream for {request_id} ending - status: {current_progress['status']}")
                            break
                            
                except Exception as e:
                    api_logger.error(f"Error in progress stream for {request_id}: {e}")
                    break
        
        except GeneratorExit:
            # Client disconnected
            api_logger.info(f"Progress stream for {request_id} closed by client")
        except Exception as e:
            api_logger.error(f"SSE connection error for {request_id}: {e}")
        finally:
            # Mark connection as inactive
            connection_active['active'] = False
            
            # Clean up connection
            if request_id in progress_connections:
                if message_queue in progress_connections[request_id]:
                    progress_connections[request_id].remove(message_queue)
                if not progress_connections[request_id]:
                    del progress_connections[request_id]
                    api_logger.info(f"All connections closed for request {request_id}")
    
    # Return Server-Sent Events response
    response = Response(
        event_generator(),
        mimetype='text/event-stream',
        headers={
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Headers': 'Cache-Control'
        }
    )
    
    return response


@app.route('/api/progress/<request_id>/info', methods=['GET'])
@log_request_response
def get_progress_info(request_id):
    """Get current progress information for a request."""
    progress_data = progress_tracker.get_request_progress(request_id)
    
    if progress_data is None:
        return jsonify({'error': 'Request not found or not being tracked'}), 404
    
    # Convert progress data to JSON-serializable format
    response_data = {
        'request_id': request_id,
        'workflow_type': progress_data['workflow_type'],
        'started_at': progress_data['started_at'],
        'status': progress_data['status'],
        'progress_percent': progress_data['progress_percent'],
        'current_step_index': progress_data['current_step_index'],
        'steps': [
            {
                'step_id': step.step_id,
                'step_name': step.step_name,
                'status': step.status,
                'agent': step.agent,
                'description': step.description,
                'started_at': step.started_at,
                'completed_at': step.completed_at,
                'progress_percent': step.progress_percent,
                'details': step.details
            }
            for step in progress_data['steps']
        ]
    }
    
    if 'finished_at' in progress_data:
        response_data['finished_at'] = progress_data['finished_at']
    
    return jsonify({
        'success': True,
        'progress': response_data
    })


@app.route('/api/initialize', methods=['POST'])
@log_request_response
def initialize_system():
    """Initialize the agent system."""
    global orchestrator, mcp_initialized
    
    try:
        if AGENT_AVAILABLE:
            print("Attempting to initialize agent system...")
            success = asyncio.run(initialize_agent_system())
            
            if success:
                return jsonify({
                    'success': True,
                    'message': 'ÂçèË∞ÉÂô®Á≥ªÁªüÂàùÂßãÂåñÊàêÂäü',
                    'orchestrator_ready': orchestrator is not None,
                    'mcp_initialized': mcp_initialized
                })
            else:
                return jsonify({
                    'success': False,
                    'message': 'ÂçèË∞ÉÂô®Á≥ªÁªüÂàùÂßãÂåñÂ§±Ë¥•ÔºåËøêË°åÂú®ÊºîÁ§∫Ê®°Âºè',
                    'orchestrator_ready': False,
                    'mcp_initialized': False
                }), 500
        else:
            return jsonify({
                'success': False,
                'message': 'AgentÁ≥ªÁªü‰∏çÂèØÁî®ÔºåÁº∫Â∞ëÂøÖË¶Å‰æùËµñ',
                'orchestrator_ready': False,
                'mcp_initialized': False
            }), 503
    
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'ÂàùÂßãÂåñÂ§±Ë¥•: {str(e)}',
            'orchestrator_ready': False,
            'mcp_initialized': False
        }), 500


@app.route('/api/upload', methods=['POST'])
@log_request_response
def upload_file():
    """Handle single file upload."""
    try:
        if 'file' not in request.files:
            return jsonify({'error': 'No file provided'}), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({'error': 'No file selected'}), 400
        
        if not allowed_file(file.filename):
            return jsonify({'error': 'Invalid file type. Please upload Excel files (.xlsx, .xls, .xlsm)'}), 400
        
        # Generate unique filename
        file_id = str(uuid.uuid4())
        filename = secure_filename(file.filename)
        file_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{file_id}_{filename}")
        
        # Save file
        file.save(file_path)
        
        # Store file info
        file_info = {
            'file_id': file_id,
            'original_name': filename,
            'file_path': file_path,
            'size': os.path.getsize(file_path),
            'upload_time': datetime.now().isoformat(),
            'processed': False
        }
        
        # Store file persistently
        stored_path = file_manager.store_file(file_id, file_path, file_info)
        file_info['file_path'] = stored_path
        uploaded_files[file_id] = file_info
        
        return jsonify({
            'success': True,
            'file_id': file_id,
            'filename': filename,
            'size': file_info['size'],
            'message': 'File uploaded successfully'
        })
        
    except Exception as e:
        return jsonify({'error': f'Upload failed: {str(e)}'}), 500


@app.route('/api/upload/batch', methods=['POST'])
@log_request_response
def upload_batch_files():
    """Handle multiple file upload."""
    try:
        if 'files' not in request.files:
            return jsonify({'error': 'No files provided'}), 400
        
        files = request.files.getlist('files')
        if not files or all(f.filename == '' for f in files):
            return jsonify({'error': 'No files selected'}), 400
        
        uploaded_file_infos = []
        file_ids = []
        
        for file in files:
            if file.filename == '':
                continue
                
            if not allowed_file(file.filename):
                api_logger.warning(f"Skipping invalid file: {file.filename}")
                continue
            
            # Generate unique filename
            file_id = str(uuid.uuid4())
            filename = secure_filename(file.filename)
            file_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{file_id}_{filename}")
            
            # Save file
            file.save(file_path)
            
            # Store file info
            file_info = {
                'file_id': file_id,
                'original_name': filename,
                'file_path': file_path,
                'size': os.path.getsize(file_path),
                'upload_time': datetime.now().isoformat(),
                'processed': False
            }
            
            # Store file persistently
            stored_path = file_manager.store_file(file_id, file_path, file_info)
            file_info['file_path'] = stored_path
            uploaded_files[file_id] = file_info
            uploaded_file_infos.append(file_info)
            file_ids.append(file_id)
        
        if not uploaded_file_infos:
            return jsonify({'error': 'No valid Excel files were uploaded'}), 400
        
        return jsonify({
            'success': True,
            'file_ids': file_ids,
            'files': [{
                'file_id': info['file_id'],
                'filename': info['original_name'],
                'size': info['size']
            } for info in uploaded_file_infos],
            'message': f'{len(uploaded_file_infos)} files uploaded successfully'
        })
        
    except Exception as e:
        return jsonify({'error': f'Batch upload failed: {str(e)}'}), 500


@app.route('/api/process/<file_id>', methods=['POST'])
@log_request_response
def process_file(file_id):
    """Process uploaded file using the agent system."""
    try:
        file_info = get_file_info_safe(file_id)
        if file_info is None:
            return jsonify({'error': 'File not found'}), 404
        
        if not AGENT_AVAILABLE or not orchestrator:
            # Perform text analysis even in mock mode
            api_logger.info(f"Performing text analysis for file {file_id}")
            text_analysis_success = file_manager.analyze_file_text(file_id, max_rows=1000)
            
            # Return mock response if agent system not available
            response_data = {
                'success': True,
                'file_id': file_id,
                'processed': True,
                'mock_mode': True,
                'steps': [
                    {'step': 'upload', 'status': 'completed', 'message': 'File uploaded'},
                    {'step': 'ingest', 'status': 'completed', 'message': 'File parsed (mock)'},
                    {'step': 'text_analysis', 'status': 'completed' if text_analysis_success else 'skipped', 
                     'message': 'Text analysis completed' if text_analysis_success else 'Text analysis skipped'},
                    {'step': 'profile', 'status': 'completed', 'message': 'Data analyzed (mock)'},
                    {'step': 'ready', 'status': 'completed', 'message': 'Ready for queries (mock)'}
                ],
                'metadata': {
                    'sheets': ['Sheet1', 'Data', 'Summary'],
                    'total_rows': 1234,
                    'total_columns': 8,
                    'file_size': file_info['size']
                },
                'preview': {
                    'columns': ['Product', 'Sales', 'Quantity', 'Date', 'Category', 'Price', 'Profit', 'Region'],
                    'sample_data': [
                        ['Laptop', 15000, 5, '2024-01-15', 'Electronics', 3000, 2000, 'North'],
                        ['Phone', 8000, 10, '2024-01-16', 'Electronics', 800, 1200, 'South'],
                        ['Tablet', 6000, 8, '2024-01-17', 'Electronics', 750, 800, 'East']
                    ]
                }
            }
            
            # Add text analysis results if available
            if text_analysis_success:
                text_analysis = file_manager.get_text_analysis(file_id)
                if text_analysis:
                    response_data['text_analysis'] = {
                        'total_texts': text_analysis.get('total_texts', 0),
                        'total_words': text_analysis.get('total_words', 0),
                        'unique_word_count': text_analysis.get('unique_word_count', 0),
                        'top_keywords': list(text_analysis.get('top_words', {}).keys())[:10]
                    }
            
            return jsonify(response_data)
        
        # Process with real agent system (direct file ingestion without intent parsing)
        async def process_with_agents():
            file_path = file_info['file_path']
            
            # Import file ingest agent and required models
            from excel_agent.agents.file_ingest import FileIngestAgent
            from excel_agent.models.agents import FileIngestRequest
            from excel_agent.models.base import AgentStatus
            
            # Create file ingest agent and process file directly
            file_agent = FileIngestAgent()
            
            ingest_request = FileIngestRequest(
                agent_id="FileIngestAgent",
                file_path=file_path,
                context={'file_id': file_id, 'processing_mode': 'file_upload'}
            )
            
            async with file_agent:
                ingest_response = await file_agent.execute_with_timeout(ingest_request)
            
            # Prepare result without intent parsing
            if ingest_response.status == AgentStatus.SUCCESS:
                return {
                    'status': 'success',
                    'workflow_type': 'file_ingest_only',
                    'steps': [
                        {'step': 'file_ingest', 'status': 'success', 'result': ingest_response.result}
                    ],
                    'file_metadata': ingest_response.result,
                    'sheets': ingest_response.sheets,
                    'file_id': ingest_response.file_id,
                    'timestamp': datetime.now().isoformat()
                }
            else:
                return {
                    'status': 'failed',
                    'error_message': 'File ingestion failed',
                    'error_log': ingest_response.error_log,
                    'timestamp': datetime.now().isoformat()
                }
        
        # Run async processing
        result = run_async(process_with_agents())
        
        # Perform text analysis for real agent processing
        api_logger.info(f"Performing text analysis for file {file_id}")
        text_analysis_success = file_manager.analyze_file_text(file_id, max_rows=1000)
        
        # Add text analysis results to the response
        if text_analysis_success:
            text_analysis = file_manager.get_text_analysis(file_id)
            if text_analysis:
                result['text_analysis'] = {
                    'total_texts': text_analysis.get('total_texts', 0),
                    'total_words': text_analysis.get('total_words', 0),
                    'unique_word_count': text_analysis.get('unique_word_count', 0),
                    'top_keywords': list(text_analysis.get('top_words', {}).keys())[:10]
                }
        
        # Update file info
        file_info['processed'] = True
        file_info['process_result'] = result
        
        return jsonify({
            'success': True,
            'file_id': file_id,
            'processed': True,
            'result': result
        })
        
    except Exception as e:
        return jsonify({'error': f'Processing failed: {str(e)}'}), 500


@app.route('/api/text-analysis/<file_id>', methods=['GET'])
@log_request_response
def get_text_analysis(file_id):
    """Get text analysis results for a file."""
    try:
        file_info = get_file_info_safe(file_id)
        if file_info is None:
            return jsonify({'error': 'File not found'}), 404
        
        text_analysis = file_manager.get_text_analysis(file_id)
        if text_analysis is None:
            return jsonify({'error': 'Text analysis not available for this file'}), 404
        
        return jsonify({
            'success': True,
            'file_id': file_id,
            'text_analysis': text_analysis
        })
        
    except Exception as e:
        return jsonify({'error': f'Failed to get text analysis: {str(e)}'}), 500


@app.route('/api/search/keywords', methods=['POST'])
@log_request_response
def search_files_by_keywords():
    """Search files by keywords in their text content."""
    try:
        data = request.get_json()
        keywords = data.get('keywords', [])
        match_any = data.get('match_any', True)
        
        if not keywords:
            return jsonify({'error': 'Keywords are required'}), 400
        
        matching_files = file_manager.search_files_by_keywords(keywords, match_any)
        
        return jsonify({
            'success': True,
            'keywords': keywords,
            'match_any': match_any,
            'matching_files': matching_files,
            'total_matches': len(matching_files)
        })
        
    except Exception as e:
        return jsonify({'error': f'Search failed: {str(e)}'}), 500


@app.route('/api/process/batch', methods=['POST'])
@log_request_response
def process_batch_files():
    """Process multiple uploaded files using the agent system."""
    try:
        data = request.get_json()
        if not data or 'file_ids' not in data:
            return jsonify({'error': 'No file IDs provided'}), 400
        
        file_ids = data['file_ids']
        if not isinstance(file_ids, list) or not file_ids:
            return jsonify({'error': 'Invalid file IDs format'}), 400
        
        # Validate all file IDs exist
        missing_files = [fid for fid in file_ids if fid not in uploaded_files]
        if missing_files:
            return jsonify({'error': f'Files not found: {missing_files}'}), 404
        
        if not AGENT_AVAILABLE or not orchestrator:
            # Return mock response if agent system not available
            processed_files = []
            for file_id in file_ids:
                file_info = uploaded_files[file_id]
                processed_files.append({
                    'file_id': file_id,
                    'filename': file_info['original_name'],
                    'processed': True,
                    'steps': [
                        {'step': 'upload', 'status': 'completed', 'message': 'File uploaded'},
                        {'step': 'ingest', 'status': 'completed', 'message': 'File parsed (mock)'},
                        {'step': 'profile', 'status': 'completed', 'message': 'Data analyzed (mock)'},
                        {'step': 'ready', 'status': 'completed', 'message': 'Ready for queries (mock)'}
                    ]
                })
                # Mark as processed
                file_info['processed'] = True
            
            return jsonify({
                'success': True,
                'processed_files': processed_files,
                'mock_mode': True,
                'message': f'{len(file_ids)} files processed successfully (mock mode)'
            })
        
        # Process with real agent system
        async def process_batch_with_agents():
            results = []
            for file_id in file_ids:
                file_info = uploaded_files[file_id]
                file_path = file_info['file_path']
                
                try:
                    # Import required components for direct file processing
                    from excel_agent.agents.file_ingest import FileIngestAgent
                    from excel_agent.models.agents import FileIngestRequest
                    from excel_agent.models.base import AgentStatus
                    
                    # Process file directly without intent parsing
                    file_agent = FileIngestAgent()
                    
                    ingest_request = FileIngestRequest(
                        agent_id="FileIngestAgent",
                        file_path=file_path,
                        context={'file_id': file_id, 'batch_processing': True}
                    )
                    
                    async with file_agent:
                        ingest_response = await file_agent.execute_with_timeout(ingest_request)
                    
                    if ingest_response.status == AgentStatus.SUCCESS:
                        result = {
                            'status': 'success',
                            'workflow_type': 'file_ingest_only',
                            'file_metadata': ingest_response.result,
                            'sheets': ingest_response.sheets,
                            'file_id': ingest_response.file_id
                        }
                    else:
                        result = {
                            'status': 'failed',
                            'error_message': 'File ingestion failed',
                            'error_log': ingest_response.error_log
                        }
                    
                    results.append({
                        'file_id': file_id,
                        'filename': file_info['original_name'],
                        'processed': True,
                        'result': result
                    })
                    
                    # Update file info
                    file_info['processed'] = True
                    file_info['process_result'] = result
                    
                except Exception as e:
                    api_logger.error(f"Failed to process file {file_id}: {str(e)}")
                    results.append({
                        'file_id': file_id,
                        'filename': file_info['original_name'],
                        'processed': False,
                        'error': str(e)
                    })
            
            return results
        
        # Run async batch processing
        results = run_async(process_batch_with_agents())
        
        # Count successful and failed processing
        successful = len([r for r in results if r.get('processed', False)])
        failed = len(results) - successful
        
        return jsonify({
            'success': True,
            'processed_files': results,
            'summary': {
                'total': len(file_ids),
                'successful': successful,
                'failed': failed
            },
            'message': f'Batch processing completed: {successful} successful, {failed} failed'
        })
        
    except Exception as e:
        return jsonify({'error': f'Batch processing failed: {str(e)}'}), 500


@app.route('/api/query', methods=['POST'])
@log_request_response
def query_data():
    """Handle user queries about the data."""
    try:
        data = request.get_json()
        file_id = data.get('file_id')
        query = data.get('query', '').strip()
        
        if not query:
            return jsonify({'error': 'Query cannot be empty'}), 400
        
        # Generate request ID for progress tracking
        query_request_id = f"query_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{id(query) % 10000}"
        
        # Start progress tracking
        progress_tracker.start_request(query_request_id, "single_table")
        
        # Get file info safely with automatic recovery
        file_info = get_file_info_safe(file_id)
        if file_info is None:
            progress_tracker.finish_request(query_request_id, "failed", "Êñá‰ª∂Êú™ÊâæÂà∞")
            return jsonify({
                'error': 'File not found',
                'message': f'File ID {file_id} does not exist or has been cleaned up',
                'suggestion': 'Please re-upload your file and try again',
                'request_id': query_request_id
            }), 404
        
        # Êô∫ËÉΩÁõ∏ÂÖ≥ÊÄßÂà§Êñ≠ - ÂÖàÈÄöËøáÂàÜËØçÂåπÈÖç
        progress_tracker.update_step(query_request_id, "relevance_analysis", "in_progress", "Ê≠£Âú®ÂàÜÊûêÊü•ËØ¢Áõ∏ÂÖ≥ÊÄß...")
        api_logger.info(f"ÂºÄÂßãÁõ∏ÂÖ≥ÊÄßÂàÜÊûê: {query}")
        text_analysis = file_manager.get_text_analysis(file_id)
        
        relevance_result = None
        enhanced_query = query
        
        if text_analysis:
            try:
                # ‰ΩøÁî®Áõ∏ÂÖ≥ÊÄßÂåπÈÖçÂô®ÂàÜÊûêÊü•ËØ¢
                relevance_result = relevance_matcher.match_query_to_sheets(query, text_analysis)
                api_logger.info(f"Áõ∏ÂÖ≥ÊÄßÂàÜÊûêÁªìÊûú: {relevance_matcher.get_relevance_summary(relevance_result)}")
                
                # Â¶ÇÊûúÊâæÂà∞ÂÖ≥ÈîÆËØçÂåπÈÖçÔºåÂ¢ûÂº∫Êü•ËØ¢‰∏ä‰∏ãÊñá
                if relevance_result.is_relevant and relevance_result.method == 'keyword_match':
                    enhanced_query = relevance_matcher.enhance_query_with_context(
                        query, 
                        relevance_result.matched_sheets, 
                        relevance_result.matched_keywords
                    )
                    api_logger.info(f"Êü•ËØ¢Â∑≤Â¢ûÂº∫: {enhanced_query}")
                
                progress_tracker.update_step(query_request_id, "relevance_analysis", "completed", 
                                           f"Áõ∏ÂÖ≥ÊÄßÂàÜÊûêÂÆåÊàê - {relevance_matcher.get_relevance_summary(relevance_result)}")
                
                # Ê£ÄÊü•ÊòØÂê¶ÂèØ‰ª•‰ΩøÁî®ËΩªÈáèÁ∫ßÂ§ÑÁêÜÊ®°ÂºèÔºàÈÄöËøámarkdownÂÜÖÂÆπÁõ¥Êé•ÂõûÁ≠îÔºâ
                lightweight_result = check_lightweight_processing(file_id, relevance_result, query_request_id, query)
                if lightweight_result:
                    api_logger.info("‰ΩøÁî®ËΩªÈáèÁ∫ßÂ§ÑÁêÜÊ®°ÂºèÔºåÁõ¥Êé•Âü∫‰∫émarkdownÂÜÖÂÆπÂõûÁ≠î")
                    return jsonify(lightweight_result)
                
            except Exception as e:
                api_logger.error(f"Áõ∏ÂÖ≥ÊÄßÂàÜÊûêÂ§±Ë¥•: {e}")
                relevance_result = None
                progress_tracker.update_step(query_request_id, "relevance_analysis", "warning", 
                                           f"Áõ∏ÂÖ≥ÊÄßÂàÜÊûêÂ§±Ë¥•: {str(e)}")
        else:
            api_logger.warning(f"Êñá‰ª∂ {file_id} Ê≤°ÊúâÊñáÊú¨ÂàÜÊûêÊï∞ÊçÆÔºåË∑≥ËøáÁõ∏ÂÖ≥ÊÄßÂåπÈÖç")
            progress_tracker.update_step(query_request_id, "relevance_analysis", "completed", 
                                       "Êó†ÊñáÊú¨ÂàÜÊûêÊï∞ÊçÆÔºåË∑≥ËøáÁõ∏ÂÖ≥ÊÄßÂåπÈÖç")

        if not AGENT_AVAILABLE or not orchestrator:
            # Return mock response if agent system not available
            # Simulate workflow steps with progress updates
            progress_tracker.update_step(query_request_id, "intent_parsing", "in_progress", "Ëß£ÊûêÊü•ËØ¢ÊÑèÂõæ...")
            time.sleep(0.5)  # Simulate processing time
            progress_tracker.update_step(query_request_id, "intent_parsing", "completed", "ÊÑèÂõæËß£ÊûêÂÆåÊàê")
            
            progress_tracker.update_step(query_request_id, "file_ingest", "in_progress", "Âä†ËΩΩÊñá‰ª∂Êï∞ÊçÆ...")
            time.sleep(0.5)
            progress_tracker.update_step(query_request_id, "file_ingest", "completed", "Êñá‰ª∂Êï∞ÊçÆÂä†ËΩΩÊàêÂäü")
            
            progress_tracker.update_step(query_request_id, "column_profiling", "in_progress", "ÂàÜÊûêÊï∞ÊçÆÂàó‰ø°ÊÅØ...")
            time.sleep(0.5)
            progress_tracker.update_step(query_request_id, "column_profiling", "completed", "Êï∞ÊçÆÂàóÂàÜÊûêÂÆåÊàê")
            
            progress_tracker.update_step(query_request_id, "code_generation", "in_progress", "ÁîüÊàêÂàÜÊûê‰ª£Á†Å...")
            mock_responses = generate_mock_response(enhanced_query, file_info)
            time.sleep(0.5)
            progress_tracker.update_step(query_request_id, "code_generation", "completed", "ÂàÜÊûê‰ª£Á†ÅÁîüÊàêÊàêÂäü")
            
            progress_tracker.update_step(query_request_id, "execution", "in_progress", "ÊâßË°åÂàÜÊûê‰ª£Á†Å...")
            time.sleep(0.5)
            progress_tracker.update_step(query_request_id, "execution", "completed", "‰ª£Á†ÅÊâßË°åÂÆåÊàê")
            
            progress_tracker.update_step(query_request_id, "response_generation", "in_progress", "ÁîüÊàêÁî®Êà∑ÂõûÁ≠î...")
            time.sleep(0.5)
            progress_tracker.update_step(query_request_id, "response_generation", "completed", "ÂõûÁ≠îÁîüÊàêÂÆåÊàê")
            
            # Create mock workflow steps with relevance info
            mock_workflow_steps = [
                {'step': 'relevance_analysis', 'status': 'success', 'agent': 'RelevanceMatcher', 
                 'description': f'Êô∫ËÉΩÁõ∏ÂÖ≥ÊÄßÂàÜÊûê: {relevance_matcher.get_relevance_summary(relevance_result) if relevance_result else "Êó†ÊñáÊú¨ÂàÜÊûêÊï∞ÊçÆ"}'},
                {'step': 'intent_parsing', 'status': 'success', 'agent': 'Orchestrator', 'description': 'ÊÑèÂõæËß£ÊûêÂÆåÊàê'},
                {'step': 'file_ingest', 'status': 'success', 'agent': 'FileIngestAgent', 'description': 'Êñá‰ª∂Êï∞ÊçÆÂä†ËΩΩÊàêÂäü'},
                {'step': 'column_profiling', 'status': 'success', 'agent': 'ColumnProfilingAgent', 'description': 'Êï∞ÊçÆÂàóÂàÜÊûêÂÆåÊàê'},
                {'step': 'code_generation', 'status': 'success', 'agent': 'CodeGenerationAgent', 'description': 'ÂàÜÊûê‰ª£Á†ÅÁîüÊàêÊàêÂäü'},
                {'step': 'execution', 'status': 'success', 'agent': 'ExecutionAgent', 'description': '‰ª£Á†ÅÊâßË°åÂÆåÊàê'}
            ]
            
            progress_tracker.finish_request(query_request_id, "completed", "ÂàÜÊûêÂÆåÊàêÔºÅ")
            
            response_data = {
                'success': True,
                'request_id': query_request_id,
                'query': query,
                'enhanced_query': enhanced_query if enhanced_query != query else None,
                'response': mock_responses['analysis'],
                'generated_code': mock_responses['code'],
                'workflow_steps': mock_workflow_steps,
                'mock_mode': True,
                'execution_time': 2.5,
                'insights': mock_responses.get('insights', []),
                'visualizations': mock_responses.get('visualizations', [])
            }
            
            # Ê∑ªÂä†Áõ∏ÂÖ≥ÊÄßÂàÜÊûêÁªìÊûú
            if relevance_result:
                response_data['relevance_analysis'] = {
                    'is_relevant': relevance_result.is_relevant,
                    'confidence_score': relevance_result.confidence_score,
                    'matched_sheets': relevance_result.matched_sheets,
                    'matched_keywords': relevance_result.matched_keywords,
                    'method': relevance_result.method,
                    'summary': relevance_matcher.get_relevance_summary(relevance_result)
                }
            
            return jsonify(response_data)
        
        # Process with real agent system
        async def query_with_agents():
            file_path = file_info['file_path']
            
            # ÂÜ≥ÂÆöÊòØÂê¶ÈúÄË¶ÅLLMËøõ‰∏ÄÊ≠•Âà§Êñ≠Áõ∏ÂÖ≥ÊÄß
            actual_query = enhanced_query
            context = {
                'file_id': file_id,
                'previous_processing': file_info.get('process_result')
            }
            
            # Ê∑ªÂä†Áõ∏ÂÖ≥ÊÄßÂàÜÊûê‰ø°ÊÅØÂà∞‰∏ä‰∏ãÊñá
            if relevance_result:
                context['relevance_analysis'] = {
                    'is_relevant': relevance_result.is_relevant,
                    'confidence_score': relevance_result.confidence_score,
                    'matched_sheets': relevance_result.matched_sheets,
                    'matched_keywords': relevance_result.matched_keywords,
                    'method': relevance_result.method
                }
                
                # Â¶ÇÊûúÂÖ≥ÈîÆËØçÂåπÈÖçÂ§±Ë¥•ÔºåÂèØËÉΩÈúÄË¶ÅLLMËøõ‰∏ÄÊ≠•Âà§Êñ≠
                if relevance_result.method == 'needs_llm_fallback':
                    context['needs_relevance_check'] = True
                    api_logger.info("ÂÖ≥ÈîÆËØçÂåπÈÖçÊú™ÊâæÂà∞Áõ∏ÂÖ≥ÊÄßÔºåÂ∞ÜÁî±LLMËøõË°åËøõ‰∏ÄÊ≠•Âà§Êñ≠")
                elif not relevance_result.is_relevant:
                    # ÊòéÁ°Æ‰∏çÁõ∏ÂÖ≥ÔºåÂèØ‰ª•ÊèêÂâçËøîÂõûÊàñËÆ©LLMÂÅöÊúÄÁªàÂà§Êñ≠
                    context['low_relevance_warning'] = True
                    api_logger.warning("Êü•ËØ¢‰∏éÊñá‰ª∂ÂÜÖÂÆπÁõ∏ÂÖ≥ÊÄßËæÉ‰Ωé")
            
            # Use orchestrator to process query
            result = await orchestrator.process_user_request(
                user_request=actual_query,
                file_path=file_path,
                context=context,
                progress_tracker=progress_tracker,
                request_id=query_request_id
            )
            
            return result
        
        # Run async query processing
        result = run_async(query_with_agents())
        
        # Check if processing was successful
        if result.get('status') == 'failed':
            progress_tracker.finish_request(query_request_id, "failed", "Â§ÑÁêÜÂ§±Ë¥•")
            api_logger.error(f"Query processing failed for file {file_id}: {result}")
            return jsonify({
                'success': False,
                'request_id': query_request_id,
                'error': result.get('error', 'Query processing failed'),
                'error_message': result.get('error_message', result.get('error', 'Unknown error')),
                'error_type': result.get('error_type', 'ProcessingError'),
                'query': query,
                'file_id': file_id,
                'execution_time': result.get('execution_time', 0),
                'workflow_steps': result.get('workflow_steps', result.get('steps', [])),
                'timestamp': result.get('timestamp'),
                'debug_info': {
                    'file_path': file_info.get('file_path'),
                    'file_processed': file_info.get('processed', False),
                    'agent_available': AGENT_AVAILABLE,
                    'orchestrator_ready': orchestrator is not None
                }
            }), 500
        
        # Mark as completed
        progress_tracker.finish_request(query_request_id, "completed", "ÂàÜÊûêÂÆåÊàêÔºÅ")
        
        # Handle different response types from orchestrator
        response_data = {
            'success': True,
            'request_id': query_request_id,
            'query': query,
            'enhanced_query': enhanced_query if enhanced_query != query else None,
            'execution_time': result.get('execution_time', 0),
            'status': result.get('status', 'completed')
        }
        
        # Ê∑ªÂä†Áõ∏ÂÖ≥ÊÄßÂàÜÊûêÁªìÊûúÂà∞ÁúüÂÆûÂìçÂ∫î
        if relevance_result:
            response_data['relevance_analysis'] = {
                'is_relevant': relevance_result.is_relevant,
                'confidence_score': relevance_result.confidence_score,
                'matched_sheets': relevance_result.matched_sheets,
                'matched_keywords': relevance_result.matched_keywords,
                'method': relevance_result.method,
                'summary': relevance_matcher.get_relevance_summary(relevance_result)
            }
        
        # Check if this is a non-Excel request response
        if result.get('response_type') == 'general_llm_response':
            # Non-Excel request: use 'answer' field and include additional metadata
            response_data.update({
                'response': result.get('answer', 'No response generated'),
                'generated_code': '',  # Non-Excel requests don't generate code
                'workflow_steps': [],  # Non-Excel requests don't have workflow steps
                'excel_data_used': result.get('excel_data_used', False),
                'file_processed': result.get('file_processed', False),
                'note': result.get('note', ''),
                'relevance_analysis': result.get('relevance_analysis', {}),
                'response_type': 'general_llm_response'
            })
        else:
            # Excel request: use existing field mapping
            response_data.update({
                'response': result.get('final_result', result.get('user_response', 'Analysis completed')),
                'generated_code': result.get('generated_code', ''),
                'workflow_steps': result.get('steps', []),
                'excel_data_used': result.get('excel_data_used', True),
                'file_processed': result.get('file_processed', True),
                'note': result.get('note', ''),
                'response_type': 'excel_data_processing'
            })
            
            # Include additional Excel workflow data if available
            if 'user_response' in result:
                response_data['user_response'] = result['user_response']
            if 'response_summary' in result:
                response_data['response_summary'] = result['response_summary']
            if 'recommendations' in result:
                response_data['recommendations'] = result['recommendations']
            if 'technical_details' in result:
                response_data['technical_details'] = result['technical_details']
        
        return jsonify(response_data)
        
    except Exception as e:
        return jsonify({'error': f'Query failed: {str(e)}'}), 500


@app.route('/api/query/batch', methods=['POST'])
@log_request_response
def query_batch_data():
    """Handle user queries about multiple files data."""
    try:
        data = request.get_json()
        file_ids = data.get('file_ids', [])
        query = data.get('query', '').strip()
        
        if not query:
            return jsonify({'error': 'Query cannot be empty'}), 400
        
        if not isinstance(file_ids, list) or not file_ids:
            return jsonify({'error': 'No file IDs provided'}), 400
        
        # Validate all file IDs exist
        missing_files = [fid for fid in file_ids if fid not in uploaded_files]
        if missing_files:
            return jsonify({'error': f'Files not found: {missing_files}'}), 404
        
        # Get file info for all files
        file_infos = [uploaded_files[fid] for fid in file_ids]
        file_names = [info['original_name'] for info in file_infos]
        
        if not AGENT_AVAILABLE or not orchestrator:
            # Return mock response for batch query
            mock_responses = generate_mock_batch_response(query, file_infos)
            
            # Create mock workflow steps
            mock_workflow_steps = [
                {'step': 'intent_parsing', 'status': 'success', 'agent': 'Orchestrator', 'description': 'ÊÑèÂõæËß£ÊûêÂÆåÊàê'},
                {'step': 'batch_file_ingest', 'status': 'success', 'agent': 'FileIngestAgent', 'description': f'{len(file_ids)}‰∏™Êñá‰ª∂Êï∞ÊçÆÂä†ËΩΩÊàêÂäü'},
                {'step': 'data_merging', 'status': 'success', 'agent': 'DataMergeAgent', 'description': 'Êï∞ÊçÆÂêàÂπ∂ÂÆåÊàê'},
                {'step': 'column_profiling', 'status': 'success', 'agent': 'ColumnProfilingAgent', 'description': 'ÊâπÈáèÊï∞ÊçÆÂàÜÊûêÂÆåÊàê'},
                {'step': 'code_generation', 'status': 'success', 'agent': 'CodeGenerationAgent', 'description': 'ÂàÜÊûê‰ª£Á†ÅÁîüÊàêÊàêÂäü'},
                {'step': 'execution', 'status': 'success', 'agent': 'ExecutionAgent', 'description': '‰ª£Á†ÅÊâßË°åÂÆåÊàê'}
            ]
            
            return jsonify({
                'success': True,
                'query': query,
                'file_ids': file_ids,
                'file_names': file_names,
                'response': mock_responses['analysis'],
                'generated_code': mock_responses['code'],
                'workflow_steps': mock_workflow_steps,
                'mock_mode': True,
                'execution_time': 4.2,
                'insights': mock_responses.get('insights', []),
                'visualizations': mock_responses.get('visualizations', [])
            })
        
        # Process with real agent system
        async def query_batch_with_agents():
            file_paths = [info['file_path'] for info in file_infos]
            
            # Âπ∂ÂèëËøõË°åÂ§öÊñá‰ª∂Áõ∏ÂÖ≥ÊÄßÊ£ÄÊµã
            import asyncio
            from backend.utils.relevance_matcher import relevance_matcher
            from backend.utils.file_manager import file_manager
            
            batch_relevance_results = {}
            
            async def check_file_relevance(file_id, file_name):
                """Ê£ÄÊü•Âçï‰∏™Êñá‰ª∂ÁöÑÁõ∏ÂÖ≥ÊÄß"""
                try:
                    api_logger.info(f"ÂºÄÂßãÊ£ÄÊü•Êñá‰ª∂ {file_name} ({file_id}) ÁöÑÁõ∏ÂÖ≥ÊÄß")
                    text_analysis = file_manager.get_text_analysis(file_id)
                    
                    if text_analysis:
                        relevance_result = relevance_matcher.match_query_to_sheets(query, text_analysis)
                        api_logger.info(f"Êñá‰ª∂ {file_name} Áõ∏ÂÖ≥ÊÄßÂàÜÊûêÁªìÊûú: {relevance_matcher.get_relevance_summary(relevance_result)}")
                        return file_id, relevance_result
                    else:
                        api_logger.warning(f"Êñá‰ª∂ {file_name} ({file_id}) Ê≤°ÊúâÊñáÊú¨ÂàÜÊûêÊï∞ÊçÆ")
                        return file_id, None
                except Exception as e:
                    api_logger.error(f"Êñá‰ª∂ {file_name} ({file_id}) Áõ∏ÂÖ≥ÊÄßÊ£ÄÊµãÂ§±Ë¥•: {e}")
                    return file_id, None
            
            # Âπ∂ÂèëÊ£ÄÊü•ÊâÄÊúâÊñá‰ª∂ÁöÑÁõ∏ÂÖ≥ÊÄß
            api_logger.info(f"ÂºÄÂßãÂπ∂ÂèëÊ£ÄÊü• {len(file_ids)} ‰∏™Êñá‰ª∂ÁöÑÁõ∏ÂÖ≥ÊÄß")
            tasks = [check_file_relevance(file_id, file_infos[i]['original_name']) for i, file_id in enumerate(file_ids)]
            relevance_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Â§ÑÁêÜÁõ∏ÂÖ≥ÊÄßÊ£ÄÊµãÁªìÊûú
            relevant_files = []
            all_matched_sheets = []
            all_matched_keywords = []
            
            for result in relevance_results:
                if isinstance(result, Exception):
                    api_logger.error(f"Áõ∏ÂÖ≥ÊÄßÊ£ÄÊµã‰ªªÂä°Â§±Ë¥•: {result}")
                    continue
                    
                file_id, relevance_result = result
                batch_relevance_results[file_id] = relevance_result
                
                if relevance_result and relevance_result.is_relevant:
                    relevant_files.append(file_id)
                    all_matched_sheets.extend(relevance_result.matched_sheets)
                    all_matched_keywords.extend(relevance_result.matched_keywords)
            
            # ÂéªÈáçÂíåÁªüËÆ°
            unique_matched_sheets = list(set(all_matched_sheets))
            unique_matched_keywords = list(set(all_matched_keywords))
            
            api_logger.info(f"ÊâπÈáèÁõ∏ÂÖ≥ÊÄßÊ£ÄÊµãÂÆåÊàê: {len(relevant_files)}/{len(file_ids)} ‰∏™Êñá‰ª∂Áõ∏ÂÖ≥")
            api_logger.info(f"ÂåπÈÖçÂ∑•‰ΩúË°®: {unique_matched_sheets[:5]}...")  # ÊòæÁ§∫Ââç5‰∏™
            api_logger.info(f"ÂåπÈÖçÂÖ≥ÈîÆËØç: {unique_matched_keywords[:10]}...")  # ÊòæÁ§∫Ââç10‰∏™
            
            # Â¶ÇÊûúÊúâÁõ∏ÂÖ≥Êñá‰ª∂Ôºå‰ºòÂåñÊü•ËØ¢‰∏ä‰∏ãÊñá
            enhanced_query = query
            if relevant_files and unique_matched_keywords:
                context_parts = []
                if unique_matched_sheets:
                    sheets_str = "„ÄÅ".join(unique_matched_sheets[:5])
                    context_parts.append(f"ÈáçÁÇπÂÖ≥Ê≥®Â∑•‰ΩúË°®Ôºö{sheets_str}")
                if unique_matched_keywords:
                    keywords_str = "„ÄÅ".join(unique_matched_keywords[:8])
                    context_parts.append(f"ÂÖ≥ÈîÆËØçÔºö{keywords_str}")
                
                if context_parts:
                    enhanced_query = f"{query}\n\n‰∏ä‰∏ãÊñáÊèêÁ§∫Ôºö{' | '.join(context_parts)}"
                    api_logger.info(f"ÊâπÈáèÊü•ËØ¢Â∑≤Â¢ûÂº∫: {enhanced_query}")
            
            # Use orchestrator to process batch query with enhanced context
            result = await orchestrator.process_user_request(
                user_request=enhanced_query,
                file_path=file_paths[0],  # Primary file
                context={
                    'file_ids': file_ids,
                    'file_paths': file_paths,
                    'file_names': file_names,
                    'batch_query': True,
                    'previous_processing': [info.get('process_result') for info in file_infos],
                    'batch_relevance_analysis': {
                        'total_files': len(file_ids),
                        'relevant_files': relevant_files,
                        'relevance_results': batch_relevance_results,
                        'matched_sheets': unique_matched_sheets,
                        'matched_keywords': unique_matched_keywords,
                        'relevance_count': len(relevant_files)
                    }
                }
            )
            
            return result
        
        # Run async batch query processing
        result = run_async(query_batch_with_agents())
        
        # ÊèêÂèñÊâπÈáèÁõ∏ÂÖ≥ÊÄßÂàÜÊûêÁªìÊûú
        batch_relevance = result.get('context', {}).get('batch_relevance_analysis', {})
        
        response_data = {
            'success': True,
            'query': query,
            'file_ids': file_ids,
            'file_names': file_names,
            'response': result.get('final_result', 'Batch analysis completed'),
            'generated_code': result.get('generated_code', ''),
            'workflow_steps': result.get('steps', []),
            'execution_time': result.get('execution_time', 0),
            'status': result.get('status', 'completed')
        }
        
        # Ê∑ªÂä†ÊâπÈáèÁõ∏ÂÖ≥ÊÄßÂàÜÊûêÁªìÊûú
        if batch_relevance:
            response_data['batch_relevance_analysis'] = {
                'total_files': batch_relevance.get('total_files', len(file_ids)),
                'relevant_files_count': batch_relevance.get('relevance_count', 0),
                'relevant_file_ids': batch_relevance.get('relevant_files', []),
                'matched_sheets': batch_relevance.get('matched_sheets', []),
                'matched_keywords': batch_relevance.get('matched_keywords', []),
                'summary': f"{batch_relevance.get('relevance_count', 0)}/{batch_relevance.get('total_files', len(file_ids))} ‰∏™Êñá‰ª∂‰∏éÊü•ËØ¢Áõ∏ÂÖ≥"
            }
            
            # ‰∏∫ÊØè‰∏™Êñá‰ª∂Ê∑ªÂä†Áõ∏ÂÖ≥ÊÄßËØ¶ÊÉÖ
            file_relevance_details = {}
            relevance_results = batch_relevance.get('relevance_results', {})
            for file_id in file_ids:
                relevance_result = relevance_results.get(file_id)
                if relevance_result:
                    file_relevance_details[file_id] = {
                        'is_relevant': relevance_result.is_relevant,
                        'confidence_score': relevance_result.confidence_score,
                        'matched_sheets': relevance_result.matched_sheets,
                        'matched_keywords': relevance_result.matched_keywords,
                        'method': relevance_result.method
                    }
                else:
                    file_relevance_details[file_id] = {
                        'is_relevant': False,
                        'confidence_score': 0.0,
                        'matched_sheets': [],
                        'matched_keywords': [],
                        'method': 'no_analysis'
                    }
            
            response_data['file_relevance_details'] = file_relevance_details
        
        return jsonify(response_data)
        
    except Exception as e:
        return jsonify({'error': f'Batch query failed: {str(e)}'}), 500


def generate_mock_batch_response(query, file_infos):
    """Generate mock responses for batch queries when agent system is not available."""
    query_lower = query.lower()
    file_count = len(file_infos)
    total_size = sum(info['size'] for info in file_infos)
    file_names = [info['original_name'] for info in file_infos]
    
    if 'ÁªüËÆ°' in query_lower or 'Âü∫Êú¨‰ø°ÊÅØ' in query_lower:
        return {
            'analysis': f"""üìä ÊâπÈáèÊï∞ÊçÆÁªüËÆ°ÂàÜÊûêÁªìÊûú

Âü∫‰∫éÊÇ®‰∏ä‰º†ÁöÑ {file_count} ‰∏™Êñá‰ª∂Ôºö
{chr(10).join([f'‚Ä¢ {name}' for name in file_names])}

üìã Êï¥‰ΩìÊï∞ÊçÆÊ¶ÇËßàÔºö
‚Ä¢ Êñá‰ª∂ÊÄªÊï∞: {file_count} ‰∏™
‚Ä¢ ÊÄªÊñá‰ª∂Â§ßÂ∞è: {total_size} bytes
‚Ä¢ ÂêàÂπ∂ÂêéÊÄªË°åÊï∞: {file_count * 1234:,} Ë°å
‚Ä¢ ÂêàÂπ∂ÂêéÊÄªÂàóÊï∞: 8 ÂàóÔºàÊ†áÂáÜÂåñÂêéÔºâ
‚Ä¢ Êï∞ÊçÆÁ±ªÂûã: 4‰∏™Êï∞ÂÄºÂàó, 3‰∏™ÊñáÊú¨Âàó, 1‰∏™Êó•ÊúüÂàó

üìà ÂêàÂπ∂Êï∞ÊçÆÁªüËÆ°Ôºö
‚Ä¢ ÈîÄÂîÆÈ¢ù: Âπ≥ÂùáÂÄº ¬•{8547 * file_count:,}, ÊúÄÂ§ßÂÄº ¬•{25000 * file_count:,}, ÊúÄÂ∞èÂÄº ¬•1,200
‚Ä¢ Êï∞Èáè: Âπ≥ÂùáÂÄº {15.6 * file_count:.1f}, ÊúÄÂ§ßÂÄº {100 * file_count}, ÊúÄÂ∞èÂÄº 1
‚Ä¢ Âà©Ê∂¶Áéá: Âπ≥ÂùáÂÄº 23.4%, Ê†áÂáÜÂ∑Æ 8.9%

üîç Êï∞ÊçÆË¥®ÈáèÔºö
‚Ä¢ ÂÆåÊï¥ÊÄß: 98.7% ({16 * file_count}‰∏™Áº∫Â§±ÂÄº)
‚Ä¢ ÈáçÂ§çËÆ∞ÂΩï: {3 * file_count}Êù°
‚Ä¢ ÂºÇÂ∏∏ÂÄº: Âú®ÈîÄÂîÆÈ¢ùÂàóÂèëÁé∞{5 * file_count}‰∏™ÊΩúÂú®ÂºÇÂ∏∏ÂÄº

üí° Ë∑®Êñá‰ª∂Ê¥ûÂØüÔºö
1. ÊâÄÊúâÊñá‰ª∂ÁöÑÁîµÂ≠ê‰∫ßÂìÅÁ±ªÂà´ÈîÄÂîÆÈ¢ùÂç†ÊÄªÈîÄÂîÆÈ¢ùÁöÑ68%
2. Êñá‰ª∂Èó¥Êï∞ÊçÆ‰∏ÄËá¥ÊÄßËâØÂ•ΩÔºåÊ†ºÂºèÁªü‰∏Ä
3. ÂêàÂπ∂ÂêéÊï∞ÊçÆÈáèÂ¢ûÂä†‰∫Ü{file_count}ÂÄçÔºåÂàÜÊûêÊõ¥ÂÖ®Èù¢
4. ÂèëÁé∞Ë∑®Êñá‰ª∂ÁöÑÊó∂Èó¥Â∫èÂàóÊ®°Âºè""",
            
            'code': f'''import pandas as pd
import numpy as np

# ËØªÂèñÂπ∂ÂêàÂπ∂Â§ö‰∏™ExcelÊñá‰ª∂
file_list = {[info['original_name'] for info in file_infos]}
all_data = []

for file in file_list:
    df = pd.read_excel(file)
    df['source_file'] = file  # Ê∑ªÂä†Ê∫êÊñá‰ª∂Ê†áËØÜ
    all_data.append(df)
    print(f"Âä†ËΩΩÊñá‰ª∂: {{file}}, Ë°åÊï∞: {{df.shape[0]}}, ÂàóÊï∞: {{df.shape[1]}}")

# ÂêàÂπ∂ÊâÄÊúâÊï∞ÊçÆ
combined_df = pd.concat(all_data, ignore_index=True)
print(f"\\nÂêàÂπ∂ÂêéÊï∞ÊçÆÂΩ¢Áä∂: {{combined_df.shape}}")

# Âü∫Êú¨ÁªüËÆ°‰ø°ÊÅØ
print("\\nÊï∞ÊçÆÁ±ªÂûã:")
print(combined_df.dtypes)

# Êï∞ÂÄºÂàóÁªüËÆ°
numeric_cols = combined_df.select_dtypes(include=[np.number]).columns
print("\\nÊï∞ÂÄºÂàóÁªüËÆ°:")
print(combined_df[numeric_cols].describe())

# ÊåâÊ∫êÊñá‰ª∂ÂàÜÁªÑÁªüËÆ°
print("\\nÂêÑÊñá‰ª∂ÁªüËÆ°:")
print(combined_df.groupby('source_file').size())

# Áº∫Â§±ÂÄºÊ£ÄÊü•
print("\\nÁº∫Â§±ÂÄºÁªüËÆ°:")
print(combined_df.isnull().sum())'''
        }
    
    else:
        return {
            'analysis': f"""ü§ñ ÊâπÈáèAIÂàÜÊûêÂõûÂ§ç

ÈíàÂØπÊÇ®ÁöÑÈóÆÈ¢òÔºö"{query}"

Âü∫‰∫é‰∏ä‰º†ÁöÑ {file_count} ‰∏™Êñá‰ª∂Ôºö
{chr(10).join([f'‚Ä¢ {name}' for name in file_names])}

üìä ÊâπÈáèÊï∞ÊçÆÊ¶ÇÂÜµÔºö
‚Ä¢ Êñá‰ª∂ÊÄªÊï∞: {file_count} ‰∏™
‚Ä¢ ÊÄªÊñá‰ª∂Â§ßÂ∞è: {total_size} bytes
‚Ä¢ È¢ÑËÆ°ÂêàÂπ∂ÂêéÊï∞ÊçÆÈáè: {file_count * 1234:,} Ë°å

üîç Âª∫ËÆÆÁöÑÊâπÈáèÂàÜÊûêÊñπÂêëÔºö
1. Ë∑®Êñá‰ª∂Êï∞ÊçÆ‰∏ÄËá¥ÊÄßÊ£ÄÊü•
2. ÂêàÂπ∂Êï∞ÊçÆÁöÑÁªüËÆ°ÂàÜÊûê
3. Êñá‰ª∂Èó¥Êï∞ÊçÆÂ∑ÆÂºÇÂØπÊØî
4. ÊâπÈáèÂºÇÂ∏∏ÂÄºÊ£ÄÊµã
5. Ë∑®Êñá‰ª∂Ë∂ãÂäøÂàÜÊûê
6. ÁªºÂêàÂèØËßÜÂåñÊä•Âëä

üí° ÊÇ®ÂèØ‰ª•Â∞ùËØï‰ª•‰∏ãÊâπÈáèÂàÜÊûêÈóÆÈ¢òÔºö
‚Ä¢ "ÂØπÊØîÂàÜÊûêËøô‰∫õÊñá‰ª∂ÁöÑÁªüËÆ°‰ø°ÊÅØ"
‚Ä¢ "Ê£ÄÊµãÊâÄÊúâÊñá‰ª∂‰∏≠ÁöÑÂºÇÂ∏∏ÂÄº"
‚Ä¢ "ÂàÜÊûêÊñá‰ª∂Èó¥ÁöÑÊï∞ÊçÆÂ∑ÆÂºÇ"
‚Ä¢ "ÁîüÊàêÁªºÂêàÊï∞ÊçÆÊä•Âëä"

ÊàëÈöèÊó∂ÂáÜÂ§á‰∏∫ÊÇ®Êèê‰æõÊõ¥ËØ¶ÁªÜÁöÑÊâπÈáèÂàÜÊûêÔºÅ""",
            
            'code': f'''import pandas as pd
import numpy as np

# ÊâπÈáèËØªÂèñExcelÊñá‰ª∂
file_list = {[info['original_name'] for info in file_infos]}
file_data = {{}}

for file in file_list:
    df = pd.read_excel(file)
    file_data[file] = df
    print(f"Êñá‰ª∂ {{file}}: {{df.shape[0]}} Ë°å x {{df.shape[1]}} Âàó")

# ÊòæÁ§∫ÊâÄÊúâÊñá‰ª∂ÁöÑÂü∫Êú¨‰ø°ÊÅØ
print(f"\\nÂÖ±Âä†ËΩΩ {{len(file_list)}} ‰∏™Êñá‰ª∂")
print(f"ÊÄªÊï∞ÊçÆÈáèÈ¢Ñ‰º∞: {{sum([df.shape[0] for df in file_data.values()]):,}} Ë°å")

# ÂêàÂπ∂ÊâÄÊúâÊï∞ÊçÆÔºàÂ¶ÇÊûúÁªìÊûÑ‰∏ÄËá¥Ôºâ
try:
    combined_df = pd.concat([df.assign(source=name) for name, df in file_data.items()], ignore_index=True)
    print(f"\\nÊï∞ÊçÆÂêàÂπ∂ÊàêÂäü: {{combined_df.shape}}")
    print(combined_df.head())
except Exception as e:
    print(f"\\nÊï∞ÊçÆÂêàÂπ∂Â§±Ë¥•: {{e}}")
    print("Êñá‰ª∂ÁªìÊûÑÂèØËÉΩ‰∏ç‰∏ÄËá¥ÔºåÈúÄË¶ÅÂçïÁã¨ÂàÜÊûê")'''
        }


def extract_sheet_markdown_content(file_id: str, sheet_names: List[str]) -> Optional[str]:
    """
    ÊèêÂèñÊåáÂÆöÂ∑•‰ΩúË°®ÁöÑmarkdownÂÜÖÂÆπ
    
    Args:
        file_id: Êñá‰ª∂ID
        sheet_names: Â∑•‰ΩúË°®ÂêçÁß∞ÂàóË°®
        
    Returns:
        ÂêàÂπ∂ÂêéÁöÑmarkdownÂÜÖÂÆπÔºåÂ¶ÇÊûúÂ§±Ë¥•ÂàôËøîÂõûNone
    """
    try:
        # Ëé∑ÂèñÂÆåÊï¥ÁöÑmarkdownÂÜÖÂÆπ
        full_markdown = file_manager.get_markdown_content(file_id)
        if not full_markdown:
            logger.warning(f"Êñá‰ª∂ {file_id} Ê≤°ÊúâmarkdownÂÜÖÂÆπ")
            return None
        
        # Â¶ÇÊûúÊ≤°ÊúâÊåáÂÆöÁâπÂÆöÂ∑•‰ΩúË°®ÔºåËøîÂõûÂÆåÊï¥ÂÜÖÂÆπ
        if not sheet_names:
            return full_markdown
        
        # ÊåâÂ∑•‰ΩúË°®ÂàÜÂâ≤markdownÂÜÖÂÆπ
        sheet_contents = []
        lines = full_markdown.split('\n')
        current_sheet = None
        current_content = []
        
        for line in lines:
            # Ê£ÄÊü•ÊòØÂê¶ÊòØÂ∑•‰ΩúË°®Ê†áÈ¢òË°å (## Sheet_Name)
            if line.startswith('## ') and len(line) > 3:
                # ‰øùÂ≠ò‰πãÂâçÁöÑÂ∑•‰ΩúË°®ÂÜÖÂÆπ
                if current_sheet and current_sheet in sheet_names:
                    sheet_contents.append('\n'.join(current_content))
                
                # ÂºÄÂßãÊñ∞ÁöÑÂ∑•‰ΩúË°®
                current_sheet = line[3:].strip()
                current_content = [line]
            else:
                # Ê∑ªÂä†ÂÜÖÂÆπÂà∞ÂΩìÂâçÂ∑•‰ΩúË°®
                current_content.append(line)
        
        # Â§ÑÁêÜÊúÄÂêé‰∏Ä‰∏™Â∑•‰ΩúË°®
        if current_sheet and current_sheet in sheet_names:
            sheet_contents.append('\n'.join(current_content))
        
        if sheet_contents:
            return '\n\n---\n\n'.join(sheet_contents)
        else:
            # Â¶ÇÊûúÊ≤°ÊúâÊâæÂà∞ÂåπÈÖçÁöÑÂ∑•‰ΩúË°®ÔºåËøîÂõûÂÆåÊï¥ÂÜÖÂÆπÁöÑ‰∏ÄÈÉ®ÂàÜ
            logger.warning(f"Êú™ÊâæÂà∞ÊåáÂÆöÂ∑•‰ΩúË°® {sheet_names}ÔºåËøîÂõûÂÆåÊï¥ÂÜÖÂÆπ")
            return full_markdown[:20000]  # ÈôêÂà∂Âú®20kÂ≠óÁ¨¶ÂÜÖ
            
    except Exception as e:
        logger.error(f"ÊèêÂèñÂ∑•‰ΩúË°®markdownÂÜÖÂÆπÂ§±Ë¥•: {e}")
        return None


def check_lightweight_processing(file_id: str, relevance_result, request_id: str, query: str = "") -> Optional[Dict[str, Any]]:
    """
    Ê£ÄÊü•ÊòØÂê¶ÂèØ‰ª•‰ΩøÁî®ËΩªÈáèÁ∫ßÂ§ÑÁêÜÊ®°ÂºèÔºàÂü∫‰∫émarkdownÂÜÖÂÆπÁõ¥Êé•ÂõûÁ≠îÔºâ
    
    Args:
        file_id: Êñá‰ª∂ID
        relevance_result: Áõ∏ÂÖ≥ÊÄßÂåπÈÖçÁªìÊûú
        request_id: ËØ∑Ê±ÇID
        
    Returns:
        Â¶ÇÊûúÂèØ‰ª•‰ΩøÁî®ËΩªÈáèÁ∫ßÊ®°ÂºèÔºåËøîÂõûÂìçÂ∫îÊï∞ÊçÆÔºõÂê¶ÂàôËøîÂõûNone
    """
    # Âè™ÊúâÂú®ÊòéÁ°ÆÂåπÈÖçÂà∞ÁâπÂÆöÂ∑•‰ΩúË°®Êó∂Êâç‰ΩøÁî®ËΩªÈáèÁ∫ßÊ®°Âºè
    if not (relevance_result and relevance_result.is_relevant and 
            relevance_result.matched_sheets and 
            relevance_result.method == 'keyword_match'):
        return None
    
    try:
        # ÊèêÂèñÂåπÈÖçÂ∑•‰ΩúË°®ÁöÑmarkdownÂÜÖÂÆπ
        progress_tracker.update_step(request_id, "lightweight_processing", "in_progress", "ÊèêÂèñÂåπÈÖçÂ∑•‰ΩúË°®ÂÜÖÂÆπ...")
        
        sheet_markdown = extract_sheet_markdown_content(file_id, relevance_result.matched_sheets)
        if not sheet_markdown:
            logger.warning("Êó†Ê≥ïÊèêÂèñÂ∑•‰ΩúË°®markdownÂÜÖÂÆπÔºåÂõûÈÄÄÂà∞ÂÆåÊï¥Â§ÑÁêÜÊ®°Âºè")
            return None
        
        # Ê£ÄÊü•ÂÜÖÂÆπÈïøÂ∫¶ÔºåÂ¶ÇÊûúË∂ÖËøá2‰∏áÂ≠óÁ¨¶Ôºå‰∏ç‰ΩøÁî®ËΩªÈáèÁ∫ßÊ®°Âºè
        if len(sheet_markdown) > 20000:
            logger.info(f"Â∑•‰ΩúË°®ÂÜÖÂÆπËøáÈïø ({len(sheet_markdown)} Â≠óÁ¨¶)ÔºåÂõûÈÄÄÂà∞ÂÆåÊï¥Â§ÑÁêÜÊ®°Âºè")
            return None
        
        logger.info(f"ÊèêÂèñÂà∞ {len(relevance_result.matched_sheets)} ‰∏™Â∑•‰ΩúË°®ÁöÑÂÜÖÂÆπÔºåÂÖ± {len(sheet_markdown)} Â≠óÁ¨¶")
        
        # ‰ΩøÁî®Ê±áÊÄªagentÂ§ÑÁêÜ
        progress_tracker.update_step(request_id, "lightweight_processing", "in_progress", "‰ΩøÁî®Ê±áÊÄª‰ª£ÁêÜÂ§ÑÁêÜÊü•ËØ¢...")
        
        if AGENT_AVAILABLE:
            # ‰ΩøÁî®ÁúüÂÆûÁöÑÊ±áÊÄªagent
            async def process_with_summarization_agent():
                from excel_agent.agents.summarization import SummarizationAgent
                from excel_agent.models.agents import SummarizationRequest
                from excel_agent.models.base import AgentStatus
                
                try:
                    summarization_agent = SummarizationAgent()
                    
                    # ÊûÑÈÄ†Ê±áÊÄªËØ∑Ê±Ç
                    summarization_request = SummarizationRequest(
                        agent_id="SummarizationAgent",
                        text_content=sheet_markdown,
                        query=query,
                        context={
                            'file_id': file_id,
                            'matched_sheets': relevance_result.matched_sheets,
                            'matched_keywords': relevance_result.matched_keywords,
                            'processing_mode': 'lightweight_markdown'
                        }
                    )
                    
                    async with summarization_agent:
                        response = await summarization_agent.execute_with_timeout(summarization_request)
                    
                    if response.status == AgentStatus.SUCCESS:
                        return {
                            'success': True,
                            'request_id': request_id,
                            'query': query,
                            'response': response.result,
                            'processing_mode': 'lightweight_markdown',
                            'matched_sheets': relevance_result.matched_sheets,
                            'matched_keywords': relevance_result.matched_keywords,
                            'content_length': len(sheet_markdown),
                            'execution_time': 0.5,
                            'workflow_steps': [
                                {'step': 'relevance_analysis', 'status': 'success', 'agent': 'RelevanceMatcher'},
                                {'step': 'content_extraction', 'status': 'success', 'agent': 'MarkdownExtractor'},
                                {'step': 'summarization', 'status': 'success', 'agent': 'SummarizationAgent'}
                            ],
                            'relevance_analysis': {
                                'is_relevant': relevance_result.is_relevant,
                                'confidence_score': relevance_result.confidence_score,
                                'matched_sheets': relevance_result.matched_sheets,
                                'matched_keywords': relevance_result.matched_keywords,
                                'method': relevance_result.method
                            }
                        }
                    else:
                        logger.error(f"Ê±áÊÄª‰ª£ÁêÜÊâßË°åÂ§±Ë¥•: {response.error_log}")
                        return None
                        
                except Exception as e:
                    logger.error(f"Ê±áÊÄª‰ª£ÁêÜÂ§ÑÁêÜÂ§±Ë¥•: {e}")
                    return None
            
            result = run_async(process_with_summarization_agent())
            if result:
                progress_tracker.update_step(request_id, "lightweight_processing", "completed", "ËΩªÈáèÁ∫ßÂ§ÑÁêÜÂÆåÊàê")
                progress_tracker.finish_request(request_id, "completed", "Êü•ËØ¢Â§ÑÁêÜÂÆåÊàêÔºÅ")
                return result
        
        # Â¶ÇÊûúagent‰∏çÂèØÁî®ÔºåËøîÂõûmockÂìçÂ∫î
        progress_tracker.update_step(request_id, "lightweight_processing", "completed", "ËΩªÈáèÁ∫ßÂ§ÑÁêÜÂÆåÊàêÔºàÊ®°ÊãüÊ®°ÂºèÔºâ")
        progress_tracker.finish_request(request_id, "completed", "Êü•ËØ¢Â§ÑÁêÜÂÆåÊàêÔºÅ")
        
        return {
            'success': True,
            'request_id': request_id,
            'query': query,
            'response': f"""Âü∫‰∫éÂåπÈÖçÂ∑•‰ΩúË°®ÁöÑÂàÜÊûêÁªìÊûúÔºö

ÂåπÈÖçÁöÑÂ∑•‰ΩúË°®Ôºö{', '.join(relevance_result.matched_sheets)}
ÂåπÈÖçÁöÑÂÖ≥ÈîÆËØçÔºö{', '.join(relevance_result.matched_keywords)}

Ê†πÊçÆÊèêÂèñÁöÑÂÜÖÂÆπÔºà{len(sheet_markdown)} Â≠óÁ¨¶ÔºâÔºåËøôÊòØ‰∏Ä‰∏™ÂÖ≥‰∫é {', '.join(relevance_result.matched_keywords)} ÁöÑÊü•ËØ¢„ÄÇ

ÂÜÖÂÆπÊëòË¶ÅÔºö
{sheet_markdown[:500]}...

Áî±‰∫éÁ≥ªÁªüÂΩìÂâçËøêË°åÂú®ÊºîÁ§∫Ê®°Âºè‰∏ãÔºåËøôÊòØ‰∏Ä‰∏™Ê®°ÊãüÂõûÁ≠î„ÄÇÂÆûÈôÖÈÉ®ÁΩ≤Êó∂‰ºö‰ΩøÁî®AIÊ±áÊÄª‰ª£ÁêÜÊèê‰æõÊõ¥ËØ¶ÁªÜÁöÑÂàÜÊûê„ÄÇ""",
            'processing_mode': 'lightweight_markdown_mock',
            'matched_sheets': relevance_result.matched_sheets,
            'matched_keywords': relevance_result.matched_keywords,
            'content_length': len(sheet_markdown),
            'mock_mode': True,
            'execution_time': 0.3,
            'workflow_steps': [
                {'step': 'relevance_analysis', 'status': 'success', 'agent': 'RelevanceMatcher'},
                {'step': 'content_extraction', 'status': 'success', 'agent': 'MarkdownExtractor'},
                {'step': 'summarization', 'status': 'success', 'agent': 'SummarizationAgent (Mock)'}
            ],
            'relevance_analysis': {
                'is_relevant': relevance_result.is_relevant,
                'confidence_score': relevance_result.confidence_score,
                'matched_sheets': relevance_result.matched_sheets,
                'matched_keywords': relevance_result.matched_keywords,
                'method': relevance_result.method
            }
        }
        
    except Exception as e:
        logger.error(f"ËΩªÈáèÁ∫ßÂ§ÑÁêÜÂ§±Ë¥•: {e}")
        progress_tracker.update_step(request_id, "lightweight_processing", "failed", f"ËΩªÈáèÁ∫ßÂ§ÑÁêÜÂ§±Ë¥•: {str(e)}")
        return None


def generate_mock_response(query, file_info):
    """Generate mock responses for testing when agent system is not available."""
    query_lower = query.lower()
    
    if 'ÁªüËÆ°' in query_lower or 'Âü∫Êú¨‰ø°ÊÅØ' in query_lower:
        return {
            'analysis': f"""üìä Êï∞ÊçÆÁªüËÆ°ÂàÜÊûêÁªìÊûú

Âü∫‰∫éÊÇ®‰∏ä‰º†ÁöÑÊñá‰ª∂ "{file_info['original_name']}"Ôºö

üìã Êï∞ÊçÆÊ¶ÇËßàÔºö
‚Ä¢ ÊÄªË°åÊï∞: 1,234 Ë°å
‚Ä¢ ÊÄªÂàóÊï∞: 8 Âàó
‚Ä¢ Êñá‰ª∂Â§ßÂ∞è: {file_info['size']} bytes
‚Ä¢ Êï∞ÊçÆÁ±ªÂûã: 4‰∏™Êï∞ÂÄºÂàó, 3‰∏™ÊñáÊú¨Âàó, 1‰∏™Êó•ÊúüÂàó

üìà Êï∞ÂÄºÂàóÁªüËÆ°Ôºö
‚Ä¢ ÈîÄÂîÆÈ¢ù: Âπ≥ÂùáÂÄº ¬•8,547, ÊúÄÂ§ßÂÄº ¬•25,000, ÊúÄÂ∞èÂÄº ¬•1,200
‚Ä¢ Êï∞Èáè: Âπ≥ÂùáÂÄº 15.6, ÊúÄÂ§ßÂÄº 100, ÊúÄÂ∞èÂÄº 1
‚Ä¢ Âà©Ê∂¶Áéá: Âπ≥ÂùáÂÄº 23.4%, Ê†áÂáÜÂ∑Æ 8.9%

üîç Êï∞ÊçÆË¥®ÈáèÔºö
‚Ä¢ ÂÆåÊï¥ÊÄß: 98.7% (16‰∏™Áº∫Â§±ÂÄº)
‚Ä¢ ÈáçÂ§çËÆ∞ÂΩï: 3Êù°
‚Ä¢ ÂºÇÂ∏∏ÂÄº: Âú®ÈîÄÂîÆÈ¢ùÂàóÂèëÁé∞5‰∏™ÊΩúÂú®ÂºÇÂ∏∏ÂÄº

üí° ÂÖ≥ÈîÆÊ¥ûÂØüÔºö
1. ÁîµÂ≠ê‰∫ßÂìÅÁ±ªÂà´ÈîÄÂîÆÈ¢ùÂç†ÊÄªÈîÄÂîÆÈ¢ùÁöÑ68%
2. Q4ÈîÄÂîÆÈ¢ùÊØîQ3Â¢ûÈïø‰∫Ü15.3%
3. Âë®‰∫îÂíåÂë®ÂÖ≠ÊòØÈîÄÂîÆÈ´òÂ≥∞Êúü""",
            
            'code': f'''import pandas as pd
import numpy as np

# ËØªÂèñExcelÊñá‰ª∂
df = pd.read_excel('{file_info['original_name']}')

# Âü∫Êú¨ÁªüËÆ°‰ø°ÊÅØ
print("Êï∞ÊçÆÂΩ¢Áä∂:", df.shape)
print("Êï∞ÊçÆÁ±ªÂûã:")
print(df.dtypes)

# Êï∞ÂÄºÂàóÁªüËÆ°
numeric_cols = df.select_dtypes(include=[np.number]).columns
print("Êï∞ÂÄºÂàóÁªüËÆ°:")
print(df[numeric_cols].describe())

# Áº∫Â§±ÂÄºÊ£ÄÊü•
print("Áº∫Â§±ÂÄºÁªüËÆ°:")
print(df.isnull().sum())'''
        }
        
    elif 'ÂºÇÂ∏∏' in query_lower or 'Á¶ªÁæ§ÁÇπ' in query_lower:
        return {
            'analysis': """üîç ÂºÇÂ∏∏ÂÄºÊ£ÄÊµãÁªìÊûú

‰ΩøÁî®IQRÊñπÊ≥ïÂíåZ-ScoreÊñπÊ≥ïÊ£ÄÊµãÂºÇÂ∏∏ÂÄºÔºö

üö® ÂèëÁé∞ÁöÑÂºÇÂ∏∏ÂÄºÔºö
‚Ä¢ ÈîÄÂîÆÈ¢ùÂàó: 5‰∏™ÂºÇÂ∏∏ÂÄº
  - Á¨¨67Ë°å: ¬•87,500 (Z-score: 4.2)
  - Á¨¨156Ë°å: ¬•0 (ÂèØËÉΩÂΩïÂÖ•ÈîôËØØ)
  - Á¨¨234Ë°å: ¬•125,000 (Z-score: 6.8)

‚Ä¢ Êï∞ÈáèÂàó: 2‰∏™ÂºÇÂ∏∏ÂÄº
  - Á¨¨123Ë°å: 999 (ÊâπÂèëËÆ¢Âçï)
  - Á¨¨567Ë°å: 0 (Èõ∂Êï∞ÈáèÂºÇÂ∏∏)

üí° Â§ÑÁêÜÂª∫ËÆÆÔºö
1. È™åËØÅÊûÅÂÄºÁöÑÁúüÂÆûÊÄß
2. Ê£ÄÊü•Èõ∂ÂÄºÂíåË¥üÂÄº
3. ËÄÉËôë‰∏öÂä°ËßÑÂàôËøõË°åÊ∏ÖÊ¥ó""",
            
            'code': '''import pandas as pd
import numpy as np
from scipy import stats

def detect_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] < lower_bound) | (df[column] > upper_bound)]

# Ê£ÄÊµãÂºÇÂ∏∏ÂÄº
outliers = detect_outliers_iqr(df, 'sales_amount')
print(f"ÂèëÁé∞ {len(outliers)} ‰∏™ÂºÇÂ∏∏ÂÄº")'''
        }
        
    elif 'Áõ∏ÂÖ≥' in query_lower or 'ÂÖ≥ËÅî' in query_lower:
        return {
            'analysis': """üîó Áõ∏ÂÖ≥ÊÄßÂàÜÊûêÁªìÊûú

Êï∞ÊçÆÁõ∏ÂÖ≥ÊÄßÂàÜÊûêÊòæÁ§∫Ôºö

üìà Âº∫Ê≠£Áõ∏ÂÖ≥ (r > 0.7):
‚Ä¢ ÈîÄÂîÆÈ¢ù ‚Üî Âà©Ê∂¶: r = 0.89
‚Ä¢ ‰ª∑Ê†º ‚Üî Âà©Ê∂¶Áéá: r = 0.76
‚Ä¢ ÂπøÂëäÊäïÂÖ• ‚Üî ÈîÄÂîÆÈ¢ù: r = 0.72

üìâ Ë¥üÁõ∏ÂÖ≥ (r < -0.3):
‚Ä¢ ÊäòÊâ£Áéá ‚Üî Âà©Ê∂¶Áéá: r = -0.65
‚Ä¢ Â∫ìÂ≠òÈáè ‚Üî ÈîÄÂîÆÈÄüÂ∫¶: r = -0.45

üí° ‰∏öÂä°Ê¥ûÂØüÔºö
1. ÈîÄÂîÆÈ¢ù‰∏éÂà©Ê∂¶È´òÂ∫¶Áõ∏ÂÖ≥ÔºåÂÆö‰ª∑Á≠ñÁï•Á®≥ÂÆö
2. ÂπøÂëäÊäïÂÖ•ROIËâØÂ•Ω
3. ÊäòÊâ£Á≠ñÁï•ÈúÄË¶Å‰ºòÂåñ""",
            
            'code': '''import seaborn as sns
import matplotlib.pyplot as plt

# ËÆ°ÁÆóÁõ∏ÂÖ≥ÊÄßÁü©Èòµ
numeric_cols = df.select_dtypes(include=[np.number]).columns
correlation_matrix = df[numeric_cols].corr()

# ÂàõÂª∫ÁÉ≠ÂäõÂõæ
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Êï∞ÊçÆÁõ∏ÂÖ≥ÊÄßÁÉ≠ÂäõÂõæ')
plt.show()'''
        }
        
    elif 'ÂõæË°®' in query_lower or 'ÂèØËßÜÂåñ' in query_lower:
        return {
            'analysis': """üìä Êï∞ÊçÆÂèØËßÜÂåñÂàÜÊûê

Â∑≤‰∏∫ÊÇ®ÁîüÊàêÂ§öÁßçÂèØËßÜÂåñÂõæË°®Ôºö

üé® ÁîüÊàêÁöÑÂõæË°®Ôºö
1. üìà ÈîÄÂîÆË∂ãÂäøÁ∫øÂõæ - ÊòæÁ§∫ÊúàÂ∫¶ÂèòÂåñ
2. üìä ‰∫ßÂìÅÁ±ªÂà´È•ºÂõæ - Á±ªÂà´ÂàÜÂ∏É
3. üìã ÈîÄÂîÆÈ¢ùÂàÜÂ∏ÉÁõ¥ÊñπÂõæ - ÈáëÈ¢ùÂàÜÂ∏É
4. üî• Áõ∏ÂÖ≥ÊÄßÁÉ≠ÂäõÂõæ - ÂèòÈáèÂÖ≥Á≥ª

üí° ÂèØËßÜÂåñÊ¥ûÂØüÔºö
‚Ä¢ ÊòéÊòæÁöÑÂ≠£ËäÇÊÄßÈîÄÂîÆÊ®°Âºè
‚Ä¢ ‰∫ßÂìÅÁ±ªÂà´ÂàÜÂ∏É‰∏çÂùáÂåÄ
‚Ä¢ Â§ßÈÉ®ÂàÜ‰∏∫‰∏≠Â∞èÈ¢ùËÆ¢Âçï
‚Ä¢ ÂèòÈáèÈó¥Â≠òÂú®ÊúâË∂£ÂÖ≥ËÅî

ÊâÄÊúâÂõæË°®Â∑≤ÁîüÊàêÔºåÂèØÁî®‰∫éÊä•ÂëäÂ±ïÁ§∫„ÄÇ""",
            
            'code': '''import matplotlib.pyplot as plt
import seaborn as sns

# ËÆæÁΩÆ‰∏≠ÊñáÂ≠ó‰Ωì
plt.rcParams['font.sans-serif'] = ['SimHei']

# ÂàõÂª∫Â≠êÂõæ
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# 1. ÈîÄÂîÆË∂ãÂäø
monthly_sales = df.groupby('month')['sales'].sum()
axes[0,0].plot(monthly_sales.index, monthly_sales.values, marker='o')
axes[0,0].set_title('ÊúàÂ∫¶ÈîÄÂîÆË∂ãÂäø')

# 2. Á±ªÂà´ÂàÜÂ∏É
category_counts = df['category'].value_counts()
axes[0,1].pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%')
axes[0,1].set_title('‰∫ßÂìÅÁ±ªÂà´ÂàÜÂ∏É')

# 3. ÈîÄÂîÆÈ¢ùÂàÜÂ∏É
axes[1,0].hist(df['sales_amount'], bins=30, alpha=0.7)
axes[1,0].set_title('ÈîÄÂîÆÈ¢ùÂàÜÂ∏É')

# 4. Áõ∏ÂÖ≥ÊÄßÁÉ≠ÂäõÂõæ
corr = df.select_dtypes(include=[np.number]).corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', ax=axes[1,1])
axes[1,1].set_title('Áõ∏ÂÖ≥ÊÄßÁÉ≠ÂäõÂõæ')

plt.tight_layout()
plt.show()'''
        }
    
    else:
        return {
            'analysis': f"""ü§ñ AIÂàÜÊûêÂõûÂ§ç

ÈíàÂØπÊÇ®ÁöÑÈóÆÈ¢òÔºö"{query}"

Âü∫‰∫é‰∏ä‰º†ÁöÑÊñá‰ª∂ "{file_info['original_name']}"ÔºåÊàëÊèê‰æõ‰ª•‰∏ãÂàÜÊûêÔºö

üìä Êï∞ÊçÆÊ¶ÇÂÜµÔºö
‚Ä¢ Êñá‰ª∂Â§ßÂ∞è: {file_info['size']} bytes
‚Ä¢ ‰∏ä‰º†Êó∂Èó¥: {file_info['upload_time']}
‚Ä¢ Â§ÑÁêÜÁä∂ÊÄÅ: {"Â∑≤Â§ÑÁêÜ" if file_info.get('processed') else "ÂæÖÂ§ÑÁêÜ"}

üîç Âª∫ËÆÆÁöÑÂàÜÊûêÊñπÂêëÔºö
1. Âü∫Êú¨ÁªüËÆ°‰ø°ÊÅØÂàÜÊûê
2. Êï∞ÊçÆË¥®ÈáèÊ£ÄÊü•
3. ÂºÇÂ∏∏ÂÄºÊ£ÄÊµã
4. Áõ∏ÂÖ≥ÊÄßÂàÜÊûê
5. Ë∂ãÂäøÂàÜÊûê
6. ÂèØËßÜÂåñÂ±ïÁ§∫

üí° ÊÇ®ÂèØ‰ª•Â∞ùËØï‰ª•‰∏ãÂÖ∑‰ΩìÈóÆÈ¢òÔºö
‚Ä¢ "ÂàÜÊûêÂü∫Êú¨ÁªüËÆ°‰ø°ÊÅØ"
‚Ä¢ "Ê£ÄÊµãÂºÇÂ∏∏ÂÄºÂíåÁ¶ªÁæ§ÁÇπ"
‚Ä¢ "ÂàÜÊûêÊï∞ÊçÆÁõ∏ÂÖ≥ÊÄß"
‚Ä¢ "ÁîüÊàêÂèØËßÜÂåñÂõæË°®"

ÊàëÈöèÊó∂ÂáÜÂ§á‰∏∫ÊÇ®Êèê‰æõÊõ¥ËØ¶ÁªÜÁöÑÂàÜÊûêÔºÅ""",
            
            'code': f'''import pandas as pd
import numpy as np

# ËØªÂèñExcelÊñá‰ª∂
df = pd.read_excel('{file_info['original_name']}')

# Âü∫Êú¨‰ø°ÊÅØ
print("Êñá‰ª∂Âü∫Êú¨‰ø°ÊÅØ:")
print(f"Ë°åÊï∞: {{df.shape[0]}}")
print(f"ÂàóÊï∞: {{df.shape[1]}}")

# Êï∞ÊçÆÈ¢ÑËßà
print("Êï∞ÊçÆÈ¢ÑËßà:")
print(df.head())

# Êï∞ÊçÆÁ±ªÂûã
print("Êï∞ÊçÆÁ±ªÂûã:")
print(df.dtypes)'''
        }


@app.route('/api/files/storage-stats')
@log_request_response
def get_storage_stats():
    """Get file storage statistics."""
    try:
        stats = file_manager.get_storage_stats()
        return jsonify({
            'success': True,
            'stats': stats,
            'memory_cache_count': len(uploaded_files),
            'persistent_storage_count': stats['file_count']
        })
    except Exception as e:
        return jsonify({'error': f'Failed to get storage stats: {str(e)}'}), 500


@app.route('/api/files')
def list_files():
    """List all uploaded files."""
    # Ensure we have the latest files from persistent storage
    try:
        stored_files = file_manager.list_files()
        uploaded_files.update(stored_files)
    except Exception as e:
        print(f"Warning: Failed to sync files from storage: {e}")
    
    files = []
    for file_id, file_info in uploaded_files.items():
        files.append({
            'file_id': file_id,
            'filename': file_info['original_name'],
            'size': file_info['size'],
            'upload_time': file_info['upload_time'],
            'processed': file_info['processed'],
            'available_formats': file_manager.get_available_formats(file_id),
            'has_markdown': file_manager.has_markdown(file_id),
            'has_html': file_manager.has_html(file_id),
            'format_conversions': file_info.get('format_conversions', {}),
            'markdown_converted_at': file_info.get('markdown_converted_at'),
            'html_converted_at': file_info.get('html_converted_at'),
            'has_text_analysis': file_manager.get_text_analysis(file_id) is not None,
            'text_analyzed_at': file_info.get('text_analyzed_at')
        })
    
    return jsonify({'files': files})


@app.route('/api/files/<file_id>', methods=['DELETE'])
@log_request_response
def delete_file(file_id):
    """Delete uploaded file."""
    try:
        file_info = get_file_info_safe(file_id)
        if file_info is None:
            return jsonify({'error': 'File not found'}), 404
        
        # Remove from persistent storage (this also deletes physical files)
        file_manager.remove_file(file_id)
        
        # Remove from memory cache
        if file_id in uploaded_files:
            del uploaded_files[file_id]
        
        return jsonify({'success': True, 'message': 'File deleted successfully'})
        
    except Exception as e:
        return jsonify({'error': f'Delete failed: {str(e)}'}), 500


@app.route('/api/files/<file_id>/formats', methods=['GET'])
@log_request_response
def get_file_formats(file_id):
    """Get available formats for a file."""
    try:
        file_info = get_file_info_safe(file_id)
        if file_info is None:
            return jsonify({'error': 'File not found'}), 404
        
        formats = file_manager.get_available_formats(file_id)
        format_details = {}
        
        for format_type in formats:
            if format_type == 'original':
                format_details['original'] = {
                    'available': True,
                    'path': file_info.get('file_path'),
                    'size': file_info.get('file_size'),
                    'type': 'original'
                }
            elif format_type == 'markdown':
                format_details['markdown'] = {
                    'available': file_manager.has_markdown(file_id),
                    'path': file_info.get('markdown_path'),
                    'converted_at': file_info.get('markdown_converted_at'),
                    'type': 'markdown'
                }
            elif format_type == 'html':
                format_details['html'] = {
                    'available': file_manager.has_html(file_id),
                    'path': file_info.get('html_path'),
                    'converted_at': file_info.get('html_converted_at'),
                    'type': 'html'
                }
        
        return jsonify({
            'success': True,
            'file_id': file_id,
            'available_formats': formats,
            'format_details': format_details
        })
        
    except Exception as e:
        return jsonify({'error': f'Failed to get file formats: {str(e)}'}), 500


@app.route('/api/files/<file_id>/content/<format_type>', methods=['GET'])
@log_request_response
def get_file_content_by_format(file_id, format_type):
    """Get file content in a specific format."""
    try:
        file_info = get_file_info_safe(file_id)
        if file_info is None:
            return jsonify({'error': 'File not found'}), 404
        
        available_formats = file_manager.get_available_formats(file_id)
        if format_type not in available_formats:
            return jsonify({'error': f'Format {format_type} not available for this file'}), 404
        
        content = None
        metadata = None
        content_type = 'application/json'
        
        if format_type == 'original':
            # For original files, return file info rather than content
            content = "Original file content (binary data)"
            content_type = 'text/plain'
        elif format_type == 'markdown':
            content = file_manager.get_markdown_content(file_id)
            metadata = file_manager.get_markdown_metadata(file_id)
            content_type = 'text/markdown'
        elif format_type == 'html':
            content = file_manager.get_html_content(file_id)
            metadata = file_manager.get_html_metadata(file_id)
            content_type = 'text/html'
        
        if content is None:
            return jsonify({'error': f'Could not retrieve content for format {format_type}'}), 500
        
        # Return as JSON response with content and metadata
        return jsonify({
            'success': True,
            'file_id': file_id,
            'format': format_type,
            'content': content,
            'metadata': metadata,
            'content_type': content_type
        })
        
    except Exception as e:
        return jsonify({'error': f'Failed to get file content: {str(e)}'}), 500


@app.route('/api/files/<file_id>/keywords', methods=['GET'])
@log_request_response
def get_file_keywords(file_id):
    """Ëé∑ÂèñÊñá‰ª∂ÁöÑÂÖ≥ÈîÆËØçÂíåÊñáÊú¨ÂàÜÊûêÁªìÊûú„ÄÇ"""
    try:
        file_info = get_file_info_safe(file_id)
        if file_info is None:
            return jsonify({'error': 'File not found'}), 404
        
        # Ëé∑ÂèñÊñáÊú¨ÂàÜÊûêÁªìÊûú
        text_analysis = file_manager.get_text_analysis(file_id)
        if not text_analysis:
            return jsonify({'error': 'Text analysis not available for this file'}), 404
        
        return jsonify({
            'success': True,
            'file_id': file_id,
            'filename': file_info.get('original_name', 'Unknown'),
            'text_analysis': text_analysis,
            'has_text_analysis': True
        })
        
    except Exception as e:
        return jsonify({'error': f'Failed to get keywords: {str(e)}'}), 500


@app.route('/api/files/<file_id>/keywords/detailed', methods=['GET'])
@log_request_response
def get_detailed_keywords(file_id):
    """Ëé∑ÂèñÊñá‰ª∂ÁöÑËØ¶ÁªÜÂÖ≥ÈîÆËØçÂàÜÊûêÁªìÊûúÔºàÂåÖÂê´ÂàÜËØç‰ø°ÊÅØÔºâ„ÄÇ"""
    try:
        file_info = get_file_info_safe(file_id)
        if file_info is None:
            return jsonify({'error': 'File not found'}), 404
        
        file_path = file_info.get('file_path')
        if not file_path:
            return jsonify({'error': 'File path not available'}), 404
        
        # Ê£ÄÊü•Êñá‰ª∂ÊòØÂê¶‰∏∫ExcelÊñá‰ª∂
        if not file_path.lower().endswith(('.xlsx', '.xls', '.xlsm')):
            return jsonify({'error': 'File is not an Excel file'}), 400
        
        # ÊâßË°åËØ¶ÁªÜÁöÑÊñáÊú¨ÂàÜÊûê
        from utils.text_processor import text_processor
        
        try:
            detailed_result = text_processor.process_excel_file(file_path, max_rows=1000)
            
            return jsonify({
                'success': True,
                'file_id': file_id,
                'filename': file_info.get('original_name', 'Unknown'),
                'detailed_analysis': detailed_result,
                'analysis_timestamp': datetime.now().isoformat()
            })
            
        except Exception as analysis_error:
            return jsonify({
                'error': f'Failed to analyze file: {str(analysis_error)}'
            }), 500
        
    except Exception as e:
        return jsonify({'error': f'Failed to get detailed keywords: {str(e)}'}), 500



@app.route('/api/preview/<file_id>', methods=['GET'])
@log_request_response
def preview_file(file_id):
    """Preview uploaded Excel file as HTML."""
    try:
        file_info = get_file_info_safe(file_id)
        if file_info is None:
            return jsonify({'error': 'File not found'}), 404
        file_path = file_info['file_path']
        
        # Get query parameters
        sheet_name = request.args.get('sheet')
        max_rows = min(int(request.args.get('max_rows', 100)), 1000)  # Cap at 1000 rows
        max_cols = min(int(request.args.get('max_cols', 20)), 50)    # Cap at 50 columns
        
        # Convert Excel to HTML
        result = excel_to_html(
            file_path=file_path,
            sheet_name=sheet_name,
            max_rows=max_rows,
            max_cols=max_cols,
            include_styling=True
        )
        
        if result['success']:
            return jsonify({
                'success': True,
                'file_id': file_id,
                'html': result['html'],
                'metadata': result['metadata'],
                'preview_data': result['preview_data']
            })
        else:
            return jsonify({
                'success': False,
                'error': result['error'],
                'html': result['html']
            }), 500
        
    except Exception as e:
        return jsonify({
            'success': False, 
            'error': f'Preview failed: {str(e)}',
            'html': f'<div class="error">Preview error: {str(e)}</div>'
        }), 500


@app.route('/api/preview/<file_id>/sheets', methods=['GET'])
@log_request_response
def get_file_sheets(file_id):
    """Get list of sheets in the Excel file."""
    try:
        file_info = get_file_info_safe(file_id)
        if file_info is None:
            return jsonify({'error': 'File not found'}), 404
        file_path = file_info['file_path']
        
        # Get Excel file information
        result = get_excel_info(file_path)
        
        if result['success']:
            return jsonify({
                'success': True,
                'file_id': file_id,
                'info': result['info']
            })
        else:
            return jsonify({
                'success': False,
                'error': result['error']
            }), 500
        
    except Exception as e:
        return jsonify({
            'success': False, 
            'error': f'Failed to get sheet info: {str(e)}'
        }), 500


@app.route('/api/preview/<file_id>/sheet/<sheet_name>', methods=['GET'])
@log_request_response  
def preview_specific_sheet(file_id, sheet_name):
    """Preview specific sheet of Excel file."""
    try:
        file_info = get_file_info_safe(file_id)
        if file_info is None:
            return jsonify({'error': 'File not found'}), 404
        file_path = file_info['file_path']
        
        # Get range parameters
        start_row = int(request.args.get('start_row', 0))
        end_row = request.args.get('end_row')
        if end_row:
            end_row = int(end_row)
        start_col = int(request.args.get('start_col', 0))  
        end_col = request.args.get('end_col')
        if end_col:
            end_col = int(end_col)
        
        # Convert specific sheet range to HTML
        result = excel_sheet_to_html(
            file_path=file_path,
            sheet_name=sheet_name,
            start_row=start_row,
            end_row=end_row,
            start_col=start_col,
            end_col=end_col
        )
        
        if result['success']:
            return jsonify({
                'success': True,
                'file_id': file_id,
                'sheet_name': sheet_name,
                'html': result['html'],
                'metadata': result['metadata']
            })
        else:
            return jsonify({
                'success': False,
                'error': result['error'],
                'html': result['html']
            }), 500
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'Sheet preview failed: {str(e)}',
            'html': f'<div class="error">Sheet preview error: {str(e)}</div>'
        }), 500


# ================================
# LLM Configuration API Endpoints
# ================================

@app.route('/api/llm/config', methods=['GET'])
@log_request_response
def get_llm_config():
    """Get current LLM configuration."""
    try:
        current_params = config.get_llm_params()
        available_models = config.get_available_models()
        
        return jsonify({
            'success': True,
            'current_params': current_params,
            'available_models': available_models,
            'parameter_limits': {
                'temperature': {'min': 0.0, 'max': 2.0, 'step': 0.1},
                'max_tokens': {'min': 1, 'max': 4000, 'step': 1},
                'top_p': {'min': 0.0, 'max': 1.0, 'step': 0.01},
                'frequency_penalty': {'min': -2.0, 'max': 2.0, 'step': 0.1},
                'presence_penalty': {'min': -2.0, 'max': 2.0, 'step': 0.1}
            }
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'Failed to get LLM config: {str(e)}'
        }), 500


@app.route('/api/llm/config', methods=['POST'])
@log_request_response
def update_llm_config():
    """Update LLM configuration."""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'error': 'No configuration data provided'}), 400
        
        # Extract valid parameters
        valid_params = {}
        for key in ['model', 'temperature', 'max_tokens', 'top_p', 'frequency_penalty', 'presence_penalty', 'stream']:
            if key in data:
                valid_params[key] = data[key]
        
        if not valid_params:
            return jsonify({'error': 'No valid parameters provided'}), 400
        
        # Update configuration
        config.update_llm_params(**valid_params)
        
        # Log the configuration change
        api_logger.info(f"üîß [Config] LLM parameters updated: {valid_params}")
        
        return jsonify({
            'success': True,
            'message': 'LLM configuration updated successfully',
            'updated_params': valid_params,
            'current_params': config.get_llm_params()
        })
        
    except ValueError as e:
        return jsonify({
            'success': False,
            'error': f'Invalid parameter value: {str(e)}'
        }), 400
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'Failed to update LLM config: {str(e)}'
        }), 500


@app.route('/api/llm/config/reset', methods=['POST'])
@log_request_response
def reset_llm_config():
    """Reset LLM configuration to default values."""
    try:
        config.reset_llm_params_to_default()
        
        api_logger.info("üîß [Config] LLM parameters reset to default values")
        
        return jsonify({
            'success': True,
            'message': 'LLM configuration reset to default values',
            'current_params': config.get_llm_params()
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'Failed to reset LLM config: {str(e)}'
        }), 500


@app.route('/api/llm/models', methods=['GET'])
@log_request_response
def get_available_models():
    """Get list of available models."""
    try:
        models = config.get_available_models()
        return jsonify({
            'success': True,
            'models': models
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'Failed to get models: {str(e)}'
        }), 500


@app.route('/api/llm/presets', methods=['GET'])
@log_request_response
def get_llm_presets():
    """Get LLM configuration presets."""
    presets = {
        'creative': {
            'name': 'ÂàõÊÑèÊ®°Âºè',
            'description': 'ÈÄÇÂêàÂàõÈÄ†ÊÄß‰ªªÂä°ÔºåÊõ¥Â§öÂèòÂåñÂíåÂàõÊñ∞',
            'params': {
                'temperature': 1.0,
                'top_p': 0.9,
                'frequency_penalty': 0.5,
                'presence_penalty': 0.3,
                'max_tokens': 2000
            }
        },
        'balanced': {
            'name': 'Âπ≥Ë°°Ê®°Âºè',
            'description': 'ÈÄÇÂêà‰∏ÄËà¨‰ªªÂä°ÔºåÂπ≥Ë°°ÂàõÈÄ†ÊÄßÂíåÂáÜÁ°ÆÊÄß',
            'params': {
                'temperature': 0.7,
                'top_p': 1.0,
                'frequency_penalty': 0.0,
                'presence_penalty': 0.0,
                'max_tokens': 1500
            }
        },
        'precise': {
            'name': 'Á≤æÁ°ÆÊ®°Âºè',
            'description': 'ÈÄÇÂêàÈúÄË¶ÅÂáÜÁ°ÆÁ≠îÊ°àÁöÑ‰ªªÂä°ÔºåÊõ¥Á°ÆÂÆöÊÄß',
            'params': {
                'temperature': 0.3,
                'top_p': 0.8,
                'frequency_penalty': 0.0,
                'presence_penalty': 0.0,
                'max_tokens': 1000
            }
        },
        'analytical': {
            'name': 'ÂàÜÊûêÊ®°Âºè',
            'description': 'ÈÄÇÂêàÊï∞ÊçÆÂàÜÊûêÂíåÈÄªËæëÊé®ÁêÜ',
            'params': {
                'temperature': 0.1,
                'top_p': 0.95,
                'frequency_penalty': 0.2,
                'presence_penalty': 0.1,
                'max_tokens': 3000
            }
        }
    }
    
    return jsonify({
        'success': True,
        'presets': presets
    })


@app.route('/api/llm/presets/<preset_name>', methods=['POST'])
@log_request_response
def apply_llm_preset(preset_name):
    """Apply a specific LLM configuration preset."""
    try:
        # Get presets
        presets_response = get_llm_presets()
        presets_data = presets_response.get_json()
        
        if not presets_data['success']:
            return jsonify({'error': 'Failed to get presets'}), 500
        
        presets = presets_data['presets']
        
        if preset_name not in presets:
            return jsonify({'error': f'Preset "{preset_name}" not found'}), 404
        
        # Apply preset parameters
        preset_params = presets[preset_name]['params']
        config.update_llm_params(**preset_params)
        
        api_logger.info(f"üîß [Config] Applied LLM preset '{preset_name}': {preset_params}")
        
        return jsonify({
            'success': True,
            'message': f'Applied preset: {presets[preset_name]["name"]}',
            'preset': presets[preset_name],
            'current_params': config.get_llm_params()
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'Failed to apply preset: {str(e)}'
        }), 500


if __name__ == '__main__':
    # Initialize agent system
    if AGENT_AVAILABLE:
        print("Attempting to initialize agent system...")
        try:
            success = asyncio.run(initialize_agent_system())
            if success:
                print("Agent system initialized successfully!")
            else:
                print("Agent system initialization failed, running in mock mode")
        except Exception as e:
            print(f"Agent system initialization error: {e}")
            print("Running in mock mode...")
    else:
        print("Agent system not available, running in mock mode")
    
    print(f"Upload folder: {app.config['UPLOAD_FOLDER']}")
    print("Starting Flask server...")
    app.run(debug=True, host='0.0.0.0', port=5000)